<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>ROCLING 2023!</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/favicon_io/favicon-32x32.png" rel="icon">
  <link href="img/favicon_io/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <style>
    .nodata {background-color:red;}
    .nochange {background-color:yellow;}
 </style>

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- Google Search Console -->
  <meta name="google-site-verification" content="-fD59RiHc4OQb37XpVtmZkcUeguaxVbKsDHtu6wsbQo" />
  
  <!-- =======================================================
    Theme Name: TheEvent
    Theme URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>
  <!--==========================
    Header
  ============================-->
  <header id="header">
    <div class="container">
      <div id="logo" class="pull-left">
        <!-- Uncomment below if you prefer to use a text logo -->

        <a href="#intro" class="scrollto"><img src="img/roclong-2023-small-logo.png" alt="" title=""></a>
      </div>

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li class="menu-active"><a href="#intro">Home</a></li>
          <!-- <li><a href="#photo">Photo</a></li> -->
          <li><a href="#call">Submit</a></li>
          <li><a href="#schedule">Program</a></li>
          <li><a href="#speakers">Keynote Speaker</a></li>
          <li><a href="#special">Special Session</a></li>
          <li><a href="#task">Shared Task</a></li>
          <li><a href="#tutorial">AI Tutorial</a></li>
          <li><a href="#buy-registrations">Reg.</a></li>
          <li><a href="#organization">Org.</a></li>
          <li><a href="#venue">Venue</a></li>
          <li><a href="history.html" target="_blank">History</a></li>
          <li class="highlight-button"><a href="https://aclanthology.org/events/rocling-2023/" target="_blank">Proceedings</a></li>
          
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->

  <!--==========================
    Intro Section
  ============================-->
  <section id="intro">
    <div class="intro-container wow fadeIn">
      <h1 class="mb-4 pb-0">The 36<sup><small>th</small></sup> Conference<br><span> on Computational Linguistics and Speech Processing </span> <br>(ROCLING 2024)</h1>
      <p class="mb-4 pb-0">November 4th - 5th , 2024 , Academia Sinica</p>
      <!---
      <a href="https://youtu.be/7f1dL0jxSs4" class="venobox play-btn mb-4" data-vbtype="video"
        data-autoplay="true"></a>
      --!>


      <a href="https://aclanthology.org/events/rocling-2023/" target="_blank" class="nodata about-btn scrollto">Click here to see proceedings</a>
      <div id="news" class="alert alert-light fade show nodata" role="alert">
        <i class="fa fa-bullhorn"></i> 2023/11/10 The presentation slides and video of some sessions have been released for all participants, please the program section.<br> 
        <i class="fa fa-bullhorn"></i> 2023/11/06 <a href="https://aclanthology.org/events/rocling-2023/" target="_blank">ROCLIGN 2023 proceedings</a> are now available on ACL Anthology.<br> 
        <i class="fa fa-bullhorn"></i> 2023/10/21 Thank you to all participants. The ROCLING 2023 conference has a large number of papers, and all of them have first authors from five different countries. Let's meet next year!<br> 
        <i class="fa fa-bullhorn"></i> 2023/10/20 Welcome to ROCLING 2023!!<br> 
        <i class="fa fa-bullhorn"></i> 2023/10/06 The late registration deadline has been extended to October 10 (Tue), 2023.<br> 
        <i class="fa fa-bullhorn"></i> 2023/09/26 The conference program has been announced.<br>
        <i class="fa fa-bullhorn"></i> 2023/09/09 Congratulations on these acceptance of papers for contribution of the ROCLING 2023! Please read the <a href="#camera-ready"><b>submission guidelines</b></a> to finish your camera-ready paper submission.<br>
        <i class="fa fa-bullhorn"></i> 2023/09/09 Accepted papers for Main Conference is posted. Please see the <a href="#accepted-papers"><b>accepted paper list</b></a>.<br>
        <i class="fa fa-bullhorn"></i> 2023/09/08 The notification of acceptance will be sent as soon as possible. Since, the review process requires a lot of time on the part of the reviewers.<br>
        <i class="fa fa-bullhorn"></i> 2023/09/01 Register today for ROLLING 2023. We look forward to seeing all of you there.<br>
        <i class="fa fa-bullhorn"></i> 2023/08/17 The final submission deadline has been extended to August 25 (Fri), 2023.<br> 
        <i class="fa fa-bullhorn"></i> <del>2023/08/10 The submission deadline has been extended to August 18 (Fri), 2023.</del><br> 
        <i class="fa fa-bullhorn"></i> 2023/08/01 The registration system is now open!<br> 
        <i class="fa fa-bullhorn"></i> 2023/07/01 The submission system is now open!<br> 
        <i class="fa fa-bullhorn"></i> 2023/06/09 The ROCLING 2023 offical website is now open! 
      </div>
    </div>
  </section>

  <main id="main">

    <!--==========================
      About Section
    ============================-->
    <section id="about">
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-lg-10">
            <h2>About The Conference</h2>
            <p>
The 36th Conference on Computational Linguistics and Speech Processing (ROCLING 2024) is the annual conference of The Association for Computational Linguistics and Chinese Language Processing (ACLCLP), a renowned international conference in the fields of natural language processing and speech processing. Held annually, it has a long history with thirty-five editions to date, covering research topics on natural language and speech signal issues and related applications. This year, it will be hosted by the Institute of Information Science, Academia Sinica, and will feature keynote speeches by experts and scholars, as well as submissions from academia and industry, rigorously peer-reviewed to present high-quality papers. Additionally, there will be a presentation of research project results funded by the National Science Council. In recent years, the development of large language models has sparked widespread interest and attention in natural language and speech processing among the public, due to their capabilities in enabling fluent dialogue, responsive answers, and understanding of questions in artificial intelligence. This edition of ROCLING looks forward to exploring the depth and breadth of research and applications in computational linguistics with interested experts, scholars, and technicians.
            </p>
          </div>

        </div>
        <div class="row justify-content-center">
          <div class="col-lg-6">
            <h3>Where</h3>
            <p>Humanities and Social Sciences Building, Conference Rooms 1 and 2, Academia Sinica</p>
          </div>
          <div class="col-lg-4">
            <h3>When</h3>
            <p>4 (Monday) to 5 (Tuesday), November 2024</p>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Important Dates Section
    ============================-->
    <section id="dates"  class="wow fadeInUp">
      <div class="container wow fadeInUp">
        <div class="section-header">
          <h2>Important Dates</h2>
          <p></p>
        </div>
        <h3 class="sub-heading"></h3>
        <div class="row justify-content-center">
          <div class="col-lg-2"></div>
          <div class="col-lg-8">
            <div class="card">
              <div class="card-body">


            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> confirmation of the organizing committee members</div>
              <div class="col-6 date">2024/01/01 – 2024/01/31 (completed) </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> invitation of keynote/invited speakers</div>
              <div class="col-6 date">2024/02/01 – 2024/04/15 (in progress) </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> evaluation and rental of conference venue</div>
              <div class="col-6 date">2024/04/01 – 2024/04/22 (completed) </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> official launch of the conference website</div>
              <div class="col-6 date">2024/04/17 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> announcement of conference promotion and call for papers</div>
              <div class="col-6 date">2024/04/20 – 2024/07/20 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> opening of submission system for papers</div>
              <div class="col-6 date">2024/05/01 – 2024/08/11 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> invitation of program committee and paper review committee members</div>
              <div class="col-6 date">2024/07/01 – 2024/08/11 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> announcement of visa application information</div>
              <div class="col-6 date">2024/07/15 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> deadline for paper submission</div>
              <div class="col-6 date">2024/08/11 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> negotiation for conference bags and souvenirs</div>
              <div class="col-6 date">2024/08/15 – 2024/08/30 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> notification of paper acceptance</div>
              <div class="col-6 date">2024/09/08 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> online conference registration</div>
              <div class="col-6 date">2024/09/01 – 2024/10/20 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> deadline for final paper submission</div>
              <div class="col-6 date">2024/09/15 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> conference program scheduling</div>
              <div class="col-6 date">2024/09/15 – 2024/09/30 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> accommodation arrangements for speakers</div>
              <div class="col-6 date">2024/09/15 – 2024/10/19 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> production and application for publication of proceedings</div>
              <div class="col-6 date">2024/09/20 – 2024/09/25 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> upload of proceedings</div>
              <div class="col-6 date">2024/09/25 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> printing of conference handbook</div>
              <div class="col-6 date">2024/10/16 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> production of large graphics for venue decoration</div>
              <div class="col-6 date">2024/10/18 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> publication of proceedings</div>
              <div class="col-6 date">2024/10/20 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> venue decoration</div>
              <div class="col-6 date">2024/11/01-03 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> conference duration</div>
              <div class="col-6 date">2024/11/04 – 2024/11/05 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> conference review meeting</div>
              <div class="col-6 date">2024/11/06 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> conference expense reimbursement</div>
              <div class="col-6 date">2024/11/06 – 2024/11/15 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> writing of conference report</div>
              <div class="col-6 date">2024/11/15 – 2024/11/30 </div>
            </div>

            <div class="row">
              <div class="col-6"><i class="fa fa-check"></i> closure</div>
              <div class="col-6 date">2024/12/1 </div>
            </div>



                <hr>
<div class="text-center text-muted">
                  <p>All deadlines are 11.59 pm UTC/GMT +08:00 (Asia/Taipei)</p>
                  <div class="d-none">
                    <div class="display-date">
                      <span>NOW</span><br>
                      <span id="day">day</span>,
                      <span id="daynum">00</span>
                      <span id="month">month</span>
                      <span id="year">0000</span>
                    </div>
                    <div class="display-time"></div>
                  </div>

                </div>
              </div>
            </div>
          </div>
          <div class="col-lg-2"></div>
        </div>
      </div>
    </section>

    <!--==========================
    Carousel
    ============================-->
    <!---
    <section id="photo" class="wow fadeInUp">
      <div class="container wow fadeInUp nodata">
        <div class="section-header">
          <h2>Photo</h2>
        </div>
        <div id="carouselExampleControls" class="carousel slide" data-bs-ride="carousel">
          <div class="carousel-inner" id="carouselInner">
            <div class="carousel-item active">
              <img src="img/photo/2T1A0583.jpg" class="d-block w-100">
            </div>
          </div>
          <button class="carousel-control-prev btn btn-outline-dark" type="button" data-target="#carouselExampleControls" data-slide="prev">
            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
          </button>
          <button class="carousel-control-next btn btn-outline-dark" type="button" data-target="#carouselExampleControls" data-slide="next">
            <span class="carousel-control-next-icon" aria-hidden="true"></span>
          </button>
        </div>
      </div>
    </section>
    --!>

    <!--==========================
      Call For Paper Section
    ============================-->
    <section id="call"  class="section-with-bg fadeInUp">
      <div class="container wow fadeInUp">
        <div class="section-header">
          <h2>Submission Guidelines</h2>
          <p></p>
        </div>
        <h3 class="sub-heading"></h3>
        <div class="row justify-content-center">
          <div class="col-lg-10">
            <p>ROCLING 2024 is one of the most influential international conferences on natural language processing and speech processing in the country. Its purpose is to promote exchange and cooperation in the fields of natural language processing and speech processing both domestically and internationally. Hosted by the Institute of Information Science, Academia Sinica, this conference will organize various academic exchange activities, including keynote speeches, academic paper presentations, seminars, workshops, etc., providing scholars, graduate students, engineers, and industry professionals with opportunities to share and discuss the latest research findings and development trends in the fields of natural language processing and speech processing. Moreover, ROCLING 2024 has invited globally renowned scholars and experts to deliver keynote speeches, sharing their latest research achievements and innovative applications in these fields. Additionally, the conference offers an ideal platform for scholars and professionals to exchange and collaborate, facilitating interaction and knowledge sharing between academia, industry, and research institutions. This edition of ROCLING 2024 will also feature a presentation session for research project outcomes funded by the National Science Council, further fostering exchange and cooperation between the industrial and academic sectors, and jointly advancing research and development in artificial intelligence technologies related to natural language and speech processing. This conference is expected to bring more inspiration and innovation to the development of related fields, while also promoting exchange and cooperation among domestic academic institutions, the industry, and the international community.</p>
            <h3>Expected Benefits</h3>
            <p>Expected Benefits By organizing academic activities such as keynote speeches, shared tasks, special sessions, paper publications, and tutorial sessions, a solid platform for academic exchange between domestic and international communities is provided, facilitating mutual exchange among the domestic academic community, industry, and the international sphere, and sparking high-quality research outcomes, enhancing research and application in the fields of speech processing and natural language processing. The expected benefits are as follows:</p>
            <ul>
                <li>Facilitate academic exchanges between domestic and international communities.</li>
                <li>Connect the research energies of industry, academia, and research institutions.</li>
                <li>Increase the visibility of domestic research outcomes in natural language and speech processing on an international level.</li>
                <li>Assist participating PhD students in enhancing the quality of their research.</li>
                <li>Enhance in-depth understanding and cooperation across disciplines.</li>
                <li>Boost the research capabilities of domestic research teams in related fields.</li>
                <li>Award annual best Master's and PhD theses to encourage newcomers to enter and develop in this field.</li>
            </ul>

            <h3>Page Limitation</h3>
            <p class="now nochange">Papers can be written and presented in either Chinese or English. Papers should be made in PDF format and submitted online through the paper submission system. Submitted papers may consist of 4-8 pages of content, plus unlimited references. Upon acceptance, final versions will be given additional pages of content (up to 9 pages) so that reviewers’ comments can be taken into account.</p>
            <h3>Relevant topics</h3>
            <p class="nodata">ROCLING 2023 mainly targets two scientific tracks: natural language processing and speech processing. The generative artificial intelligence topic is also welcomed.</p>
            <div class="row nodata">
              <div class="col-lg-6">
                <h4>Natural Language Processing</h4>
                <ul>
                  <li>Cognitive/Psychological Linguistics</li>
                  <li>Discourse and Pragmatics</li>
                  <li>Dialogue System</li>
                  <li>Information Extraction</li>
                  <li>Information Retrieval</li>
                  <li>Language Generation</li>
                  <li>Machine Translation</li>
                  <li>NLP Applications</li>
                  <li>Phonology, Morphology and Word Segmentation</li>
                  <li>Question Answering</li>
                  <li>Resources and Evaluation</li>
                  <li>Semantics: Lexical, Sentence-Level, Textual Inference</li>
                  <li>Sentiment Analysis</li>
                  <li>Summarization</li>
                  <li>Syntax: Tagging, Chunking and Parsing</li>
                  <li>Others</li>
                </ul>
              </div>
              <div class="col-lg-6">
                <h4>Speech Processing</h4>
                <ul>
                  <li>Speech Perception, Production and Acquisition</li>
                  <li>Phonetics, Phonology and Prosody</li>
                  <li>Analysis of Paralinguistics in Speech and Language</li>
                  <li>Speaker and Language Identification</li>
                  <li>Analysis of Speech and Audio Signals</li>
                  <li>Speech Coding and Enhancement</li>
                  <li>Speech Synthesis and Spoken Language Generation</li>
                  <li>Speech Recognition</li>
                  <li>Spoken Dialog Systems and Analysis of Conversation</li>
                  <li>Spoken Language Processing: Retrieval, Translation, Summarization, Resources and Evaluation</li>
                  <li>Others</li>
                </ul>
              </div>
            </div>
            <h3>Online submission system</h3>
            <p class="nodata">Paper submissions must use the official ROCLING 2023 style templates (Latex and Word) and <a href="assets/rocling2023-templates.zip">download here</a>. Submission is electronic, using the EasyChair conference management system. The submission site is available at <a href="https://easychair.org/conferences/?conf=rocling2023" target="_blank">https://easychair.org/conferences/?conf=rocling2023</a></p>
            <p class="nochange">As the reviewing will be double-blind, papers must not include authors' names and affiliations. Furthermore, self-references that reveal the author's identity must be avoided. Papers that do not conform to these requirements will be rejected without review. Papers may be accompanied by a resource (software and/or data) described in the paper, but these resources should be anonymized as well.</p>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Accepted Papers Section
    ============================-->
    <section class="accordion" id="accepted-papers" class="wow fadeInUp">
      <div class="container wow fadeInUp">
        <div class="section-header">
          <h2>Accepted Papers</h2>
          <p></p>
        </div>
        <div class="row justify-content-center nochange">
          <div class="col-lg-10">
            <p>Congratulations on these acceptance of papers for presentation at the ROCLING 2023! We appreciate your contribution to the conference and look forward to your participation. We appreciate your contribution to the conference and look forward to your participation.</p>
            <button class="btn btn-link btn-block text-left" type="button" data-toggle="collapse" data-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne"><h4>ORAL Presentation <small>click to see more</small></h4>
            </button>
            <div id="collapseOne" class="collapse" aria-labelledby="headingOne" data-parent="#accepted-papers">
              <ul>
                <li><span class="title">Construction of Message Deliver Service Dialog Systems</span> (#1)<br><span class="authors">Cheng-Hung Yeh and Chia-Hui Chang</span></li>
                <li><span class="title">Sentence-level Revision with Neural Reinforcement Learning</span> (#3)<br><span class="authors">Zhendong Du and Kenji Hashimoto</span></li>
                <li><span class="title">Multimodal Speech Training for the Hard of Hearing in Mandarine</span> (#4)<br><span class="authors">慶祥 何, 坤川 曾 and 曼菁 雷</span></li>
                <li><span class="title">Is GPT-4 a Good Islamic Expert for Answering Quran Questions?</span> (#5)<br><span class="authors">Sarah Alnefaie, Eric Atwell and Mohammad Ammar Alsalka</span></li>
                <li><span class="title">Auxiliary loss to attention head for end to end speaker diarization</span> (#6)<br><span class="authors">Yi-Ting Yang, Jiun-Ting Li and Berlin Chen</span></li>
                <li><span class="title">XFEVER: Exploring Fact Verification across Languages</span> (#7)<br><span class="authors">Yi-Chen Chang, Canasai Kruengkrai and Junichi Yamagishi</span></li>
                <li><span class="title">Enhancing Automated English Speaking Assessment for L2 Speakers with BERT and Wav2vec2.0 Fusion</span> (#8)<br><span class="authors">Wen-Hsuan Peng, Hsin-Wei Wang, Sally Chen and Berlin Chen</span></li>
                <li><span class="title">Analyzing ChatGPT's Mathematical Deficiencies: Insights and Contributions</span> (#9)<br><span class="authors">Vincent Cheng and Yu Zhang</span></li>
                <li><span class="title">Taiwanese/Mandarin Speech Recognition using OpenAI's Whisper Multilingual Speech Recognition Engine Based on Generative Pretrained Transformer Architecture</span> (#10)<br><span class="authors">Yueh-Che Hsieh and Renyuan Lyu</span></li>
                <li><span class="title">An Analysis of “X shi Y” Metaphors in Mandarin Corpora and Learning Materials</span> (#11)<br><span class="authors">Yu-Hsiang Shen and Siaw-Fong Chung</span></li>
                <li><span class="title">The Pilot Study and Model Construction for Word Segmentation in Taiwan Hakka</span> (#12)<br><span class="authors">Chiou-Shing Yeh, Huei-Ling Lai and Jyi-Shane Liu</span></li>
                <li><span class="title">Compact CNNs for End-to-End Keyword Spotting on Resource-Constrained Edge AI Devices</span> (#15)<br><span class="authors">Joseph Lin and Renyuan Lyu</span></li>
                <li><span class="title">Improving End-to-end  Taiwanese-Speech-to-Chinese-Text Translation by Semi-supervised Learning</span> (#17)<br><span class="authors">Yu-Chun Lin, Chung-Che Wang and Jyh-Shing Jang</span></li>
                <li><span class="title">Addressing  the issue of Data Imbalance in Multi-granularity Pronunciation Assessment</span> (#18)<br><span class="authors">Meng-Shin Lin, Hsin-Wei Wang, Tien-Hong Lo, Berlin Chen and Wei-Cheng Chao</span></li>
                <li><span class="title">Category Mapping for Zero-shot Text Classification</span> (#20)<br><span class="authors">Qiu-Xia Zhang, Te-Yu Chi, Te-Lun Yang, Yu-Meng Tang, Ta-Lin Chen and Jyh-Shing Jang</span></li>
                <li><span class="title">Sound Processing for Cochlear Implants: The Journey of Innovation Toward Artificial Intelligence</span> (#21)<br><span class="authors">Enoch Hsin-Ho Huang, Chao-Min Wu and Yu Tsao</span></li>
                <li><span class="title">ESC MA-SD Net: Effective Speaker Separation through Convolutional Multi-View Attention and SudoNet</span> (#22)<br><span class="authors">Che-Wei Liao, Aye Nyein Aung and Jeih-Weih Hung</span></li>
                <li><span class="title">Leveraging Dialogue Discourse Parsing in a Two-Stage Framework for Meeting Summarization</span> (#23)<br><span class="authors">Yi-Ping Huang, Tien-Hong Lo and Berlin Chen</span></li>
                <li><span class="title">Improving Low-Resource Speech Recognition through Multilingual Fine-Tuning with Language Identifiers and Self-Training</span> (#24)<br><span class="authors">Karol Nowakowski and Michal Ptaszynski</span></li>
                <li><span class="title">Story Co-telling Dialogue Generation via Reinforcement Learning and Knowledge Graph</span> (#25)<br><span class="authors">聿鎧 李 and Chia-Hui Chang</span></li>
                <li><span class="title">SCU-MESCLab at ROCLING-2023 Shared Task：Named Entity Recognition Using Multiple Classifier Model</span> (#27 MultiNER-Health)<br><span class="authors">Ruei-Cyuan Su, Tzu-En Su, Tsung-Hsien Yang and Ming-Hsiang Su</span></li>
                <li><span class="title">AaWLoss: An Artifact-aware Weighted Loss Function for Speech Enhancement</span> (#30)<br><span class="authors">En-Lun Yu, Kuan-Hsun Ho and Berlin Chen</span></li>
                <li><span class="title">A Comparative Study of Generative Pre-trained Transformer-based  Models for Chinese Slogan Generation of Crowdfunding</span> (#32)<br><span class="authors">Yu-Cheng Liang, Meng-Heng Zheng and Jheng-Long Wu</span></li>
                <li><span class="title">Application of Deep Learning Technology to Predict Changes in Sea Level</span> (#34)<br><span class="authors">Yi-Lin Hsieh and Ming-Hsiang Su</span></li>
                <li><span class="title">WordRank: A Word Ranking based Training Strategy for Abstractive Document Summarization</span> (#35)<br><span class="authors">Hsiao-Wei Chou, Pingyen Wu, Jia-Jang Tu and Kuanyu Chen</span></li>
                <li><span class="title">Overview of the ROCLING 2023 Shared Task for Chinese Multi-genre Named Entity Recognition in the Healthcare Domain</span> (#36 MultiNER-Health)<br><span class="authors">Lung-Hao Lee, Tzu-Mi Lin and Chao-Yi Chen</span></li>
                <li><span class="title">CrowNER at ROCLING 2023 MultiNER-Health Task: Enhancing NER Task with GPT Paraphrase Augmentation on Sparsely Labeled Data</span> (#37 MultiNER-Health)<br><span class="authors">Yin-Chieh Wang, Wen-Hong Wu, Feng-Yu Kuo, Han-Chun Wu, Te-Yu Chi, Te-Lun Yang, Sheh Chen and Jyh-Shing Roger Jang</span></li>
                <li><span class="title">ISLab  at  ROCLING  2023 MultiNER-Health Task:  A Three-Stage NER Model Combining  Textual Content and Tagged Semantics</span> (#40 MultiNER-Health)<br><span class="authors">Jun-Jie Wu, Yu-Cheng Liu, Tao-Hsing Chang and Fu-Yuan Hsu</span></li>
                <li><span class="title">Fine-Tuning and Evaluation of Question Generation for Slovak Language</span> (#41)<br><span class="authors">Ondrej Megela, Daniel Hladek, Matus Pleva, Ján Staš, Ming-Hsian Su and Yuan-Fu Liao</span></li>
                <li><span class="title">The Relevance Identification Between Housing Rental Texts And Legal Provisions</span> (#43)<br><span class="authors">Min-Chin Ho, Ya-Mien Cheng and Jheng-Long Wu</span></li>
                <li><span class="title">Impact of Feature Selection Algorithms on Readability Model</span> (#44)<br><span class="authors">采寧 戴, 厚強 曾 and 曜廷 宋</span></li>
                <li><span class="title">Investigating Cross-Institutional Recognition of Cancer Registration Items: A Case Study on Catastrophic Forgetting</span> (#46)<br><span class="authors">You Chen Zhang, Hong-Jie Dai and Chen-Kai Wang</span></li>
                <li><span class="title">Phonotactic Constraints on Zhangzhou Onsets</span> (#47)<br><span class="authors">Yishan Huang</span></li>
                <li><span class="title">Accelerating Hakka Speech Recognition Research and Development Using the Whisper Model</span> (#48 Hakka-ASR)<br><span class="author">Ching-Yuan Chen, Yun-Hsiang Hsu and Chenchi Chang</span></li>
                <li><span class="title">Enhancing Automatic Speech Recognition Performance Through Multi-Speaker Text-to-Speech</span> (#49 Hakka-ASR)<br><span class="author">Po-Kai Chen, Bing-Jhih Huang, Chi-Tao Chen, Hsin-Min Wang and Jia-Ching Wang</span></li>
                <li><span class="title">The DMS-ASR System for the Formosa Speech Recognition Challenge 2023</span> (#50 Hakka-ASR)<br><span class="author">Hsiu Jui Chang and Wei Yuan Chen</span></li>
                <li><span class="title">NSYSU-MITLab Speech Recognition System for Formosa Speech Recognition Challenge 2023</span> (#51 Hakka-ASR)<br><span class="author">Hong-Jie Hu and Chia-Ping Chen</span></li>
                <li><span class="title">The North System for Formosa Speech Recognition Challenge 2023</span> (#52 Hakka-ASR)<br><span class="author">Li-Wei Chen, Hung-Shin Lee and Kai-Chen Cheng</span></li>
                <li><span class="title">WhisperHakka: A Hybrid Architecture Speech Recognition System for Low-Resource Taiwanese Hakka</span> (#53 Hakka-ASR)<br><span class="author">Ming-Hsiu Chiang, Chien-Hung Lai and Hsuan-Sheng Chiu</span></li>
                <li><span class="title">The NTNU ASR System for Formosa Speech Recognition Challenge 2023</span> (#54 Hakka-ASR)<br><span class="author">Hao-Chien Lu, Chung-Chun Wang, Jhen-Ke Lin and Tien-Hong Lo</span></li>
                <li><span class="title">The Taiwan AI Labs Hakka ASR System for Formosa Speech Recognition Challenge 2023</span> (#55 Hakka-ASR)<br><span class="author">Yuan-Hsiang Lu, Chung-Yi Li and Zih-Wei Lin</span></li>
                <li><span class="title">A Preliminary Study on Hakka Speech Recognition by Using the Branchformer</span> (#56 Hakka-ASR)<br><span class="author">Jia-Jyu Su, Dong-Min Li and Chen-Yu Chiang</span></li>
                <li><span class="title">The NTNU Super Monster Team (SPMT) system for the Formosa Speech Recognition Challenge 2023 - Hakka ASR</span> (#57 Hakka-ASR)<br><span class="author">Tzu-Ting Yang, Hsin Wei Wang, Meng-Ting Tsai and Berlin Chen</span></li>
                <li><span class="title">Whisper Model Adaptation for FSR-2023 Hakka Speech Recognition Challenge</span> (#58 Hakka-ASR)<br><span class="author">Yi-Chin Huang and Ji-Qian Tsai</span></li>
              </ul>
            </div>
            
            <button class="btn btn-link btn-block text-left" type="button" data-toggle="collapse" data-target="#collapseSecond" aria-expanded="true" aria-controls="collapseSecond"><h4>POSTER Presentation <small>click to see more</small></h4>
            </button>
            <div id="collapseSecond" class="collapse" aria-labelledby="headingSencond" data-parent="#accepted-papers">
              <ul>
                <li><span class="title">KNOT-MCTS: An Effective Approach to Addressing Hallucinations in Generative Language Modeling for Question Answering</span> (#13)<br><span class="authors">Chung-Wen Wu, Guan-Tang Huang, Yue-Yang He and Berlin Chen</span></li>
                <li><span class="title">Can generative models be used to detect hate speech related to body shaming?</span> (#14)<br><span class="authors">元翔 蔡 and 瑜芸 張</span></li>
                <li><span class="title">A Novel Named Entity Recognition Model Applied to Specialized Sequence Labeling</span> (#16)<br><span class="authors">Ruei-Cyuan Su, Tzu-En Su, Ming-Hsiang Su, Matus Pleva and Daniel Hladek</span></li>
                <li><span class="title">Analysis of Chinese Irony on PTT Corpus-Using "Tested Positive" and "Hope" as the Key Words</span> (#19)<br><span class="authors">品文 王 and 曉芳 鍾</span></li>
                <li><span class="title">Evaluating Interfaced LLM Bias</span> (#26)<br><span class="authors">Kai-Ching Yeh, Jou-An Chi and Da-Chen Lian</span></li>
                <li><span class="title">YNU-HPCC at ROCLING 2023 MultiNER-Health Task: A transformer-based approach for Chinese healthcare NER</span> (#28 MultiNER-Health)<br><span class="authors">Chonglin Pang, You Zhang and Xiaobing Zhou</span></li>
                <li><span class="title">YNU-ISE-ZXW at ROCLING 2023 MultiNER-Health Task: A Transformer-based Model with LoRA for Chinese Healthcare Named Entity Recognition</span> (#29 MultiNER-Health)<br><span class="authors"></span>Xingwei Zhang, Jin Wang and Xuejie Zhang</li>
                <li><span class="title">Analyzing Bid-Rigging Related Judicial Cases of Government Procurement Law Using Text Mining Techniques</span> (#31)<br><span class="authors">Pei-Zhen Chen, Hsin-Yun Hsu and Jheng-Long Wu</span></li>
                <li><span class="title">Fine-Grained Argument Understanding with BERT Ensemble Techniques:  A Deep Dive into Financial Sentiment Analysis</span> (#33)<br><span class="authors">Sy Eugene L., Tzu-Cheng Peng, Shih-Hsuan Huang, Hen-You Lin and Yung-Chun Chang</span></li>
                <li><span class="title">Wangxuelin at ROCLING 2023 MultiNER-Health Task: Intelligent Capture of Chinese Medical Named Entities by LLM: Instruct-Tuning via Entity Extraction-Style Prompts</span> (#38 MultiNER-Health)<br><span class="authors">Xuelin Wang and Qihao Yang</span></li>
                <li><span class="title">Lexical Complexity Prediction using Word Embeddings</span> (#39)<br><span class="authors">Cheng-Zen Yang, Jin-Jian Li and Shu-Chang Lin</span></li>
                <li><span class="title">Solving Linguistic Olympiad Problems with Tree-of-Thought Prompting</span> (#45)<br><span class="authors">Zheng-Lin Lin, Chiao-Han Yen, Jia-Cheng Xu, Deborah Watty and Shu-Kai Hsieh</span></li>
              </ul>
            </div>            
          </div>
        </div>
       
      </div>
    </section>

    <!--==========================
      Camera-Ready Paper Section
    ============================-->
    <section id="camera-ready"  class="section-with-bg fadeInUp">
      <div class="container wow fadeInUp nochange">
        <div class="section-header">
          <h2>Camera-Ready Paper Submission Guidelines</h2>
          <p></p>
        </div>
        <h3 class="sub-heading"></h3>
        <div class="row justify-content-center">
          <div class="col-lg-10">
            <p>To ensure a smooth and successful publication process, please adhere to the following camera-ready paper submission guidelines.</p>
            <h4>Important Dates</h4>
            <ul >
              <li>Camera-ready paper submission deadline: September 15 (Fri), 2023</li>
              <li>Registration deadline: September 15 (Fri), 2023</li>
            </ul>
            <h4>Paper Length</h4>
            <p>Your camera-ready paper should not exceed 9 pages (plus unlimited references) in the PDF format.</p>
            <h4>Paper Format</h4>
            <p>The template is compatible with popular document preparation systems such as LaTeX and Microsoft Word. Please strictly follow the formatting guidelines provided in the <a href="assets/rocling2023-templates.zip">final template</a></p>
            <h4>File Format</h4>
            <p>Submit your camera-ready paper in PDF format.</p>
            <h4>Paper Content</h4>
            <p>The first page of the camera-ready version of the accepted paper should bear the items of paper title, author name, affiliation, and email address.</p>
            <p>All these items should be properly centered on the top, followed by a concise abstract of the paper. In addition, Chinese manuscript must to write English title, abstract, and keywords.</p>
            <h4>Author Information</h4>
            <p>Ensure that the author list in your paper is final and correct, as it will be used for conference materials and proceedings. Changes to the author list after submission will not be allowed.</p>
            <h4>Copyright Forms</h4>
            <p>Authors must sign and upload a copyright document, granting the conference organizers the right to publish the paper in the conference proceedings. The copyright form is available for download <a href="assets/rocling2023_copyright.doc"> here</a>.</p>
            <h4>Submission Instructions</h4>
            <p>Camera-ready papers should be submitted electronically through the conference submission system, which can be accessed at <a href="https://easychair.org/conferences/?conf=rocling2023" target="_blank">https://easychair.org/conferences/?conf=rocling2023</a>. Please log in using your existing account and follow the instructions for paper submission. Each item on the forms in submission system must be written in English.</p>
            <h4>Plagiarism Check</h4>
            <p>Camera-ready paper must to do plagiarism detection by authorself. Ensure that your submission is original and properly cited.</p>
            <h4>Registration</h4>
            <p>At least one author must pay the regular registration fee for each publication paper (including shared tasks). More registration information, please see at <a href="#buy-registrations">here</a></p>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Schedule Section
    ============================-->
    <section id="schedule" class="wow fadeInUp">
      <div class="container wow fadeInUp">
        <div class="section-header">
          <h2>Programs</h2>
          <p></p>
        </div>
        <h3 class="sub-heading"></h3>
        



	<div class="row">
          <ul class="nav nav-tabs" role="tablist">
            <li class="nav-item">
              <h5><a class="nav-link active" href="#day-1" role="tab" data-toggle="tab">Day 1 (Monday), 4 November 2024</a></h5>
            </li>
          </ul>
	</div>
	<div class="tab-content row justify-content-center">
          <div role="tabpanel" class="col-11 tab-pane fade show active" id="day-1">

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>08:30 – 17:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Registration <span>Location: 3rd floor of hssb </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>09:00 – 09:20 </time></div>
              <div class="col-md-10">
                
                    <h4>Opening Ceremony <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>09:20 – 10:20 </time></div>
              <div class="col-md-10">
                
                    <h4>NLP Keynote: Prof. Eduard Hovy <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>10:20 – 10:40 </time></div>
              <div class="col-md-10">
                
                    <h4>Coffee Break <span>Location: 3rd floor of HSSB </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>10:40 – 12:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Panel Discussion <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>12:00 – 13:30        </time></div>
              <div class="col-md-10">
                
                    <h4>Lunch (4F Lobby B) / ACLCLP Assembly <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>13:30 – 15:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Oral Session I <br/> Speech and Language Processing-1 <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                
                    <h4>Shared Task <br/> Chinese Named Entity Recognition <span>Location: 2nd Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>15:00 – 15:30 </time></div>
              <div class="col-md-10">
                
                    <h4>Coffee Break <span>Location: 3rd floor of HSSB </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>15:30 – 17:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Oral Session II <br/> Information Retrieval and Text Mining <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                
                    <h4>Special Session I <br/> Speech and Language <span>Location: 2nd Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>18:00 – 20:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Banquet </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            
        </div>
        </div>
        <br><br><br><br>

	<div class="row">
          <ul class="nav nav-tabs" role="tablist">
            <li class="nav-item">
              <h5><a class="nav-link active" href="#day-2" role="tab" data-toggle="tab">Day 2 (Tuesday), 5 November 2024</a></h5>
            </li>
          </ul>
	</div>
	<div class="tab-content row justify-content-center">
          <div role="tabpanel" class="col-11 tab-pane fade show active" id="day-2">

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>09:00 – 17:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Registration <span>Location: 3rd floor of hssb </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>09:20 – 10:20 </time></div>
              <div class="col-md-10">
                
                    <h4>Speech Keynote: Prof. Heng Ji <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>10:20 – 10:40 </time></div>
              <div class="col-md-10">
                
                    <h4>Coffee Break <span>Location: 3rd floor of HSSB </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>10:40 – 12:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Oral Session III <br/> Best Paper Candidates <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>12:00 – 14:00 </time></div>
              <div class="col-md-10">
                
                    <h4>Lunch / Posters / 國科會計畫成果展 <span>Location: 3rd floor of HSSB </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>14:00 – 15:30 </time></div>
              <div class="col-md-10">
                
                    <h4>Oral Session IV <br/> Speech and Language Processing-2 <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                
                    <h4>Oral Session V <br/> Applications <span>Location: 2nd Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>15:30 – 15:50 </time></div>
              <div class="col-md-10">
                
                    <h4>Coffee Break <span>Location: 3rd floor of HSSB </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>15:50 – 17:20 </time></div>
              <div class="col-md-10">
                
                    <h4>Oral Session VI <br/> Dimensional Sentiment Analysis for Educational Texts <span>Location: 1st Conference Room </span> </h4>
                    
                    

                    <hr>
                
                    <h4>ChatGPT Tutorial <span>Location: 2nd Conference Room </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            

            <div class="row schedule-item">
              <div class="col-md-2"><time>17:20 – 17:40 </time></div>
              <div class="col-md-10">
                
                    <h4>Closing Ceremony and Best Paper Award </span> </h4>
                    
                    

                    <hr>
                
                    <h4> <span>Location: R0101 </span> </h4>
                    
                    

                    <hr>
                

              </div>
            </div>

            
        </div>
        </div>
        <br><br><br><br>






        <br><br><br><br>

        
        <!-- Schdule papers -->
        <div class="row justify-content-center">
          <div class="col-lg-10 detailed-programs">
            <!-- Modal Session 1 -->
            <div class="modal fade" id="modal-session-1" tabindex="-1" aria-labelledby="modal-session-1Label" aria-hidden="true">
              <div class="modal-dialog modal-xl modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-1Label">Oral Session 1: Best Paper Award Candidates</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">Auxiliary Loss to Attention Head for End to End Speaker Diarization</span> (#6)<br><span class="author">Yi-Ting Yang, Jiun-Ting Li and Berlin Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">XFEVER: Exploring Fact Verification across Languages</span> (#7)<br><span class="author">Yi-Chen Chang, Canasai Kruengkrai and Junichi Yamagishi</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Improving End-to-End Taiwanese-Speech-to-Chinese-Text Translation by Semi-Supervised Learning</span> (#17)<br><span class="author">Yu-Chun Lin, Chung-Che Wang and Jyh-Shing Jang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Improving Low-Resource Speech Recognition through Multilingual Fine-Tuning with Language Identifiers and Self-Training</span> (#24)<br><span class="author">Karol Nowakowski and Michal Ptaszynski</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Story Co-Telling Dialogue Generation via Reinforcement Learning and Knowledge Graph</span> (#25)<br><span class="author">Yu-Kai Lee and Chia-Hui Chang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">WordRank: A Word Ranking based Training Strategy for Abstractive Document Summarization</span> (#35)<br><span class="author">Hsiao-Wei Chou, Ping-Yen Wu, Jia-Jang Tu and Kuanyu Chen</span></li>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session 2 -->
            <div class="modal fade" id="modal-session-2" tabindex="-1" aria-labelledby="modal-session-2Label" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-2Label">Oral Session 2: NLP Applications</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">Sentence-Level Revision with Neural Reinforcement Learning</span> (#3)<br><span class="author">Zhendong Du and Kenji Hashimoto</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Is GPT-4 a Good Islamic Expert for Answering Quran Questions?</span> (#5)<br><span class="author">Sarah Alnefaie, Eric Atwell and Mohammad Ammar Alsalka</span></li>
                      <br>
                      <li class="text-muted"><span class="title">An Analysis of “X shi Y” Metaphors in Mandarin Corpora and Learning Materials</span> (#11)<br><span class="author">Yu-Hsiang Shen and Siaw-Fong Chung</span></li>
                      <br>
                      <li class="text-muted"><span class="title">A Comparative Study of Generative Pre-Trained Transformer-Based  Models for Chinese Slogan Generation of Crowdfunding</span> (#32)<br><span class="author">Yu-Cheng Liang, Meng-Heng Zheng and Jheng-Long Wu</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Impact of Feature Selection Algorithms on Readability Model</span> (#44)<br><span class="author">Tsai-Ning Tai, Hou-Chiang Tseng and Yao-Ting Sung</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session 3 -->
            <div class="modal fade" id="modal-session-3" tabindex="-1" aria-labelledby="modal-session-3Label" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-3Label">Oral Session 3: Thesis Award Winners Presentation</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">持續學習閘門調適器應用於雙語神經編解碼器語音合成</span><br><span class="author">楊立任</span></li>
                      <br>
                      <li class="text-muted"><span class="title">協同式對比學習於假設領域適應</span><br><span class="author">葉宜萍</span></li>
                      <br>
                      <li class="text-muted"><span class="title">外掛式語言模型：利用一個簡單的迴歸模型控制文本生成</span><br><span class="author">楊奈其</span></li>
                      <br>
                      <li class="text-muted"><span class="title">利用學習語句表達改進中文斷詞對於不同標準的適應性</span><br><span class="author">林峻毅</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session 4 -->
            <div class="modal fade" id="modal-session-4" tabindex="-1" aria-labelledby="modal-session-4Label" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-4Label">Oral Session 4: Speech Applications</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">Construction of Message Deliver Service Dialog Systems</span> (#1)<br><span class="author">Cheng-Hung Yeh and Chia-Hui Chang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Enhancing Automated English Speaking Assessment for L2 Speakers with BERT and Wav2vec2.0 Fusion</span> (#8)<br><span class="author">Wen-Hsuan Peng, Hsin-Wei Wang, Sally Chen and Berlin Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">The Pilot Study and Model Construction for Word Segmentation in Taiwan Hakka</span> (#12)<br><span class="author">Chiou-Shing Yeh, Huei-Ling Lai and Jyi-Shane Liu</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Addressing the Issue of Data Imbalance in Multi-Granularity Pronunciation Assessment</span> (#18)<br><span class="author">Meng-Shin Lin, Hsin-Wei Wang, Tien-Hong Lo, Berlin Chen and Wei-Cheng Chao</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Application of Deep Learning Technology to Predict Changes in Sea Level</span> (#34)<br><span class="author">Yi-Lin Hsieh and Ming-Hsiang Su</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Fine-Tuning and Evaluation of Question Generation for Slovak Language</span> (#41)<br><span class="author">Ondrej Megela, Daniel Hladek, Matus Pleva, Ján Staš, Ming-Hsiang Su and Yuan-Fu Liao</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session 5 -->
            <div class="modal fade" id="modal-session-5" tabindex="-1" aria-labelledby="modal-session-5Label" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-5Label">Oral Session 5: Speech Recognition</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">Multimodal Speech Training for the Hard of Hearing in Mandarine</span> (#4)<br><span class="author">Ching-Hsiang Ho, Kun-Chuan Tseng and Men-Ching Lei</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Taiwanese/Mandarin Speech Recognition using OpenAI's Whisper Multilingual Speech Recognition Engine Based on Generative Pretrained Transformer Architecture</span> (#10)<br><span class="author">Yueh-Che Hsieh, Renyuan Lyu and Keming Lyu</span></li>
                      <br>
                      <li class="text-muted"><span class="title">ESC MA-SD Net: Effective Speaker Separation through Convolutional Multi-View Attention and SudoNet</span> (#22)<br><span class="author">Che-Wei Liao, Aye Nyein Aung and Jeih-Weih Hung</span></li>
                      <br>
                      <li class="text-muted"><span class="title">AaWLoss: An Artifact-Aware Weighted Loss Function for Speech Enhancement</span> (#30)<br><span class="author">En-Lun Yu, Kuan-Hsun Ho and Berlin Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Investigating Cross-Institutional Recognition of  Cancer Registration Items: A Case Study on Catastrophic Forgetting</span> (#46)<br><span class="author">You Chen Zhang, Chen-Kai Wang, Ming-Ju Tsai and Hong-Jie Dai</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Phonotactic Constraints on Zhangzhou Onsets</span> (#47)<br><span class="author">Yishan Huang</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session 6 -->
            <div class="modal fade" id="modal-session-6" tabindex="-1" aria-labelledby="modal-session-6Label" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-6Label">Oral Session 6: Information Retrieval

</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">Analyzing ChatGPT's Mathematical Deficiencies: Insights and Contributions</span> (#9)<br><span class="author">Vincent Cheng and Yu Zhang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Compact CNNs for End-to-End Keyword Spotting on Resource-Constrained Edge AI Devices</span> (#15)<br><span class="author">Joseph Lin and Renyuan Lyu</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Category Mapping for Zero-Shot Text Classification</span> (#20)<br><span class="author">Qiu-Xia Zhang, Te-Yu Chi, Te-Lun Yang, Yu-Meng Tang, Ta-Lin Chen and Jyh-Shing Jang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Sound Processing for Cochlear Implants: The Journey of Innovation Toward Artificial Intelligence</span> (#21)<br><span class="author">Enoch Hsin-Ho Huang, Chao-Min Wu and Yu Tsao</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Leveraging Dialogue Discourse Parsing in a Two-Stage Framework for Meeting Summarization</span> (#23)<br><span class="author">Yi-Ping Huang, Tien-Hong Lo and Berlin Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">The Relevance Identification Between Housing Rental Texts And Legal Provisions</span> (#43)<br><span class="author">Min-Chin Ho, Ya-Mien Cheng and Jheng-Long Wu</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session shared task 1 -->
            <div class="modal fade" id="modal-session-st1" tabindex="-1" aria-labelledby="modal-session-st1Label" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-st1Label">Oral Session (Shared Task 1): MultiNER-Health</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">SCU-MESCLab at ROCLING-2023 Shared Task：Named Entity Recognition Using Multiple Classifier Model</span> (#27 MultiNER-Health)<br><span class="author">Tzu-En Su, Ruei-Cyuan Su, Tsung-Hsien Yang and Ming-Hsiang Su</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Overview of the ROCLING 2023 Shared Task for Chinese Multi-Genre Named Entity Recognition in the Healthcare Domain</span> (#36 MultiNER-Health)<br><span class="author">Lung-Hao Lee, Tzu-Mi Lin and Chao-Yi Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">CrowNER at ROCLING 2023 MultiNER-Health Task: Enhancing NER Task with GPT Paraphrase Augmentation on Sparsely Labeled Data</span> (#37 MultiNER-Health)<br><span class="author">Yin-Chieh Wang, Wen-Hong Wu, Feng-Yu Kuo, Han-Chun Wu, Te-Yu Chi, Te-Lun Yang, Sheh Chen and Jyh-Shing Roger Jang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">ISLab at ROCLING 2023 MultiNER-Health Task: A Three-Stage NER Model Combining Textual Content and Tagged Semantics</span> (#40 MultiNER-Health)<br><span class="author">Jun-Jie Wu, Tao-Hsing Chang and Fu-Yuan Hsu</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session shared task 2 -->
            <div class="modal fade" id="modal-session-st2" tabindex="-1" aria-labelledby="modal-session-st2Label" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-st2Label">Oral Session (Shared Task 2): Hakka ASR </h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">Accelerating Hakka Speech Recognition Research and Development Using the Whisper Model</span> (#48 Hakka-ASR)<br><span class="author">Ching-Yuan Chen, Yun-Hsiang Hsu and Chenchi Chang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Enhancing Automatic Speech Recognition Performance Through Multi-Speaker Text-to-Speech</span> (#49 Hakka-ASR)<br><span class="author">Po-Kai Chen, Bing-Jhih Huang, Chi-Tao Chen, Hsin-Min Wang and Jia-Ching Wang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">The DMS-ASR System for the Formosa Speech Recognition Challenge 2023</span> (#50 Hakka-ASR)<br><span class="author">Hsiu Jui Chang and Wei Yuan Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">NSYSU-MITLab Speech Recognition System for Formosa Speech Recognition Challenge 2023</span> (#51 Hakka-ASR)<br><span class="author">Hong-Jie Hu and Chia-Ping Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">The North System for Formosa Speech Recognition Challenge 2023</span> (#52 Hakka-ASR)<br><span class="author">Li-Wei Chen, Hung-Shin Lee and Kai-Chen Cheng</span></li>
                      <br>
                      <li class="text-muted"><span class="title">WhisperHakka: A Hybrid Architecture Speech Recognition System for Low-Resource Taiwanese Hakka</span> (#53 Hakka-ASR)<br><span class="author">Ming-Hsiu Chiang, Chien-Hung Lai and Hsuan-Sheng Chiu</span></li>
                      <br>
                      <li class="text-muted"><span class="title">The NTNU ASR System for Formosa Speech Recognition Challenge 2023</span> (#54 Hakka-ASR)<br><span class="author">Hao-Chien Lu, Chung-Chun Wang, Jhen-Ke Lin and Tien-Hong Lo</span></li>
                      <br>
                      <li class="text-muted"><span class="title">The Taiwan AI Labs Hakka ASR System for Formosa Speech Recognition Challenge 2023</span> (#55 Hakka-ASR)<br><span class="author">Yuan-Hsiang Lu, Chung-Yi Li and Zih-Wei Lin</span></li>
                      <br>
                      <li class="text-muted"><span class="title">A Preliminary Study on Hakka Speech Recognition by Using the Branchformer</span> (#56 Hakka-ASR)<br><span class="author">Jia-Jyu Su, Dong-Min Li and Chen-Yu Chiang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">The NTNU Super Monster Team (SPMT) system for the Formosa Speech Recognition Challenge 2023 - Hakka ASR</span> (#57 Hakka-ASR)<br><span class="author">Tzu-Ting Yang, Hsin Wei Wang, Meng-Ting Tsai and Berlin Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Whisper Model Adaptation for FSR-2023 Hakka Speech Recognition Challenge</span> (#58 Hakka-ASR)<br><span class="author">Yi-Chin Huang and Ji-Qian Tsai</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal Session poster -->
            <div class="modal fade" id="modal-session-poster" tabindex="-1" aria-labelledby="modal-session-posterLabel" aria-hidden="true">
              <div class="modal-dialog modal-lg modal-dialog-centered modal-dialog-scrollable">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="modal-session-posterLabel">Poster Session</h5>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <ul>
                      <li class="text-muted"><span class="title">KNOT-MCTS: An Effective Approach to Addressing Hallucinations in Generative Language Modeling for Question Answering</span> (#13)<br><span class="author">Chung-Wen Wu, Guan-Tang Huang, Yue-Yang He and Berlin Chen</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Can Generative Models be used to Detect Hate Speech related to Body Shaming?</span> (#14)<br><span class="author">Yuan-Shiang Tsai and Yu-Yun Chang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">A Novel Named Entity Recognition Model Applied to Specialized Sequence Labeling</span> (#16)<br><span class="author">Ruei-Cyuan Su, Tzu-En Su, Ming-Hsiang Su, Matus Pleva and Daniel Hladek</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Analysis of Chinese Irony on PTT Corpus-Using "Tested Positive" and "Hope" as the Key Words</span> (#19)<br><span class="author">品文 王 and 曉芳 鍾</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Evaluating Interfaced LLM Bias</span> (#26)<br><span class="author">Kai-Ching Yeh, Jou-An Chi, Da-Chen Lian and Shu-Kai Hsieh</span></li>
                      <br>
                      <li class="text-muted"><span class="title">YNU-HPCC at ROCLING 2023 MultiNER-Health Task: A Transformer-Based Approach for Chinese healthcare NER</span> (#28)<br><span class="author">Chonglin Pang, You Zhang and Xiaobing Zhou</span></li>
                      <br>
                      <li class="text-muted"><span class="title">YNU-ISE-ZXW at ROCLING 2023 MultiNER-Health Task: A Transformer-Based Model with LoRA for Chinese Healthcare Named Entity Recognition</span> (#29)<br><span class="author">Xingwei Zhang, Jin Wang and Xuejie Zhang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Analyzing Bid-Rigging Related Judicial Cases of Government Procurement Law Using Text Mining Techniques</span> (#31)<br><span class="author">Pei-Zhen Chen, Hsin-Yun Hsu and Jheng-Long Wu</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Fine-Grained Argument Understanding with BERT Ensemble Techniques: A Deep Dive into Financial Sentiment Analysis</span> (#33)<br><span class="author">Eugene Sy, Tzu-Cheng Peng, Shih-Hsuan Huang, Heng-Yu Lin and Yung-Chun Chang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">LingX at ROCLING 2023 MultiNER-Health Task: Intelligent Capture of Chinese Medical Named Entities by LLMs</span> (#38)<br><span class="author">Xuelin Wang and Qihao Yang</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Lexical Complexity Prediction using Word Embeddings</span> (#39)<br><span class="author">Cheng-Zen Yang, Jin-Jian Li and Shu-Chang Lin</span></li>
                      <br>
                      <li class="text-muted"><span class="title">Solving Linguistic Olympiad Problems with Tree-of-Thought Prompting</span> (#45)<br><span class="author">Zheng-Lin Lin, Chiao-Han Yen, Jia-Cheng Xu, Deborah Watty and Shu-Kai Hsieh</span></li>
                      <br>
                    </ul>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <br><br><br><br>

        <!-- Presentation Guidelines  -->
        <div id="presentationguidelines" class="row justify-content-center">
          <div class="col-lg-10 nochange">
            <h3><b>Presentation Guidelines</b></h3>
            <h4>For Oral Presentations:</h4>
            <ul>
              <li>For oral presentations, we recommend downloading the ROCLING 2023 <a href="https://rocling2023.github.io/assets/rocling2023-slider-template.pptx">slider templates</a>.</li>
              <li>Presentations may be conducted in either English or Chinese.</li>
              <li>Each presentation will be allocated 20 minutes for the presentation itself, included a 4-minute session for questions and answers, and an additional 1-minute break for a change of speakers.</li>
              <li>Presenters are kindly requested to introduce themselves to the session chairs before the commencement of their oral session.</li>
              <li>Each room will be equipped with:
                <ul>
                  <li>A laptop computer (Windows OS), which can load PPT and PDF</li>
                  <li>A projector</li>
                  <li>A shared Internet connection</li>
                  <li>an audio system</li>
                  <li>The display connectors for the screen are both HDMI and VGA.</li>
                </ul>
              </li>
              <li>Presenters who want to use their laptop for their presentation must bring their adapter to connect to the HDMI/VGA cable and any audio connectors if they have a non-standard audio-out port.</li>
              <li>Before the session, presenters should inform the session chair and test that their computer and adapter work with the projector in the room.</li>
              <li>A wireless internet connection will be available in the presentation rooms.</li>
            </ul>
            <h4>For Poster Presentations:</h4>
            <ul>
              <li>Posters are in A1 size (59.4 cm wide x 84.1 cm high, or 23.4 inches x 33.1 inches).</li>
              <li>Presenters are advised to mount their posters before the start of the session and dismount them after the end of the session.</li>
              <li>Materials to fix the posters will be available on-site.</li>
            </ul>
            <h4>Pre-recorded Video Instructions (Expats Only):</h4>
            <p></p>
            <ul>
              <li>In exceptional circumstances (e.g., when international speakers (foreign business travelers) cannot present live), pre-recorded presentations for oral or poster may be permitted and played during the conference.</li>
              <li>At least 15 minutes and at most 20 minutes. Within that interval, choose a duration you feel will best engage your audience. These include having a video of the presenter in the corner of the slides.</li>
              <li>The maximum volume of video is 200MB.</li>
              <li>The file format of video is *.mp4.</li>
              <li>We recommended a video resolution of at least 720 pixels and an aspect ratio of 16:9.</li>
              <li>Please note that final specifications of the pre-recorded video will be checked, non-compliant video may be requested to be re-recorded.</li>
              <li>Please sent the download link (e.g. Google Drive or Dropbox) of the pre-recorded video to the official email (rocling2023@gmail.com) by October 15. In case of poster sessions, the poster file must also be sent in the same time.</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Speakers Section
    ============================-->
    <section id="speakers" class="section-with-bg fadeInUp">
      <div class="container nodata">
        <div class="section-header">
          <h2>Keynote Speakers</h2>
        </div>
        <h3 id="speakers-1" class="text-muted">Natural Language Processing</h3>
        <div class="row">
          <div class="col-8 col-sm-5 col-md-4 col-lg-3">
            <div class="speaker">
              <img src="img/keynote/NancyFChen.png" alt="Speaker Nancy F. Chen" class="img-fluid">
              <div class="details">
                <h3><a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-nancy-f-chen" target="_blank">Nancy F. Chen</a></h3>
                <p>A*STAR</p>
                <div class="social">
                  <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-nancy-f-chen" target="_blank"><i class="fa fa-info-circle"></i></a>
                </div>
              </div>
            </div>
          </div>
          <div class="col">
            <h3><b><I>SeaEval</I> for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning</b></h3>
            <p></p>
          </div>
        </div>
        <div class="row">
          <div class="col text-justify">
            <br>
            <h4><b>Abstract</b></h4>
            <p>We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we examine the brittleness of foundation models in the dimensions of semantics and multilinguality. Our investigations encompasses both open-source and proprietary models, shedding light on their behavior in classic NLP tasks, reasoning, and cultural contexts. Notably, (1) Most models respond inconsistently to paraphrased instructions. (2) Exposure bias pervades, evident in both standard NLP tasks and cultural understanding. (3) For questions rooted in factual, scientific, or common sense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, many models intriguingly demonstrate inconsistent performance on such queries. (4) Models trained multilingually still lack ``balanced multilingual'' capabilities. Our endeavors underscore the need for more generalizable semantic representations and enhanced multilingual contextualization. SeaEval can serve as a launchpad for in-depth investigations for multilingual and multicultural evaluations.</p>
            <h4><b>Biography</b></h4>
            <p>Nancy F. Chen is an A*STAR fellow, senior principal scientist, principal investigator, and group leader at I<sup>2</sup>R (Institute for Infocomm Research) and Principal Investigator at CFAR (Centre for Frontier AI Research). Her group works on generative AI in speech, language, and conversational technology. Her research has been applied to education, defense, healthcare, and media/journalism. Dr. Chen has published 100+ papers and supervised 100+ students/staff. She has won awards from IEEE, Microsoft, NIH, P&G, UNESCO, L’Oréal, SIGDIAL, APSIPA, MICCAI. She is an IEEE SPS Distinguished Lecturer (2023-2024), Program Chair of ICLR 2023, Board Member of ISCA (2021-2025), and Singapore 100 Women in Tech (2021). Technology from her team has led to commercial spin-offs and government deployment. Prior to A*STAR, she worked at MIT Lincoln Lab while doing a PhD at MIT and Harvard. For more info: http://alum.mit.edu/www/nancychen.</p>
          </div>
        </div>
        <br><br>
        <hr>
        <h3 id="speakers-2" class="text-muted">Speech Processing</h3>
        <div class="row">
          <div class="col-8 col-sm-5 col-md-4 col-lg-3">
            <div class="speaker">
              <img src="img/keynote/Peng-Jen.jpg" alt="Speaker Peng-Jen Chen" class="img-fluid">
              <div class="details">
                <h3><a href="https://www.linkedin.com/in/chen-peng-jen-a7a56227">Peng-Jen Chen</a></h3>
                <p>Meta AI</p>
                <div class="social">
                  <a href="https://www.linkedin.com/in/chen-peng-jen-a7a56227" target="_blank"><i class="fa fa-linkedin"></i></a>
                </div>
              </div>
            </div>
          </div>
          <div class="col">
            <h3><b>Building Speech-to-Speech Translation System for English-Hokkien</b></h3>
            <p></p>
          </div>
        </div>
        <div class="row">
          <div class="col text-justify">
            <br>
            <h4><b>Abstract</b></h4>
            <p>Speech is the primary mode of communication for people who speak languages that lack a standard writing system. With nearly 3000 such unwritten languages in existence, developing speech-to-speech translation technology is critical in overcoming language barriers for these communities. In this talk, we will explore the challenges involved in building a speech-to-speech translation system for English-Taiwanese Hokkien, a real-world language that lacks a widely used standard writing system. We will present our approaches ranging from training data collection and modeling choices, to the evaluation of the developed models.</p>
            <h4><b>Biography</b></h4>
            <p>Peng-Jen Chen is a research engineer at Meta AI. He received a B.S. degree in 2007 and an M.S. degree in 2009 in Computer Science and Information Engineering, at National Taiwan University. He joined Meta as a machine learning engineer in 2012 and joined FAIR as a research engineer in 2018. His key research interests include low-resource machine translation, speech-to-speech translation, speech-text joint pre-training.</p>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Special Session Section
    ============================-->
    <section id="special" class="wow fadeInUp">
      <div class="container nodata">
        <div class="section-header">
          <h2>Special Session</h2>
          <p>ROCLING 2024 will feature two special sessions that provide a novel topics for participants to know the trend on large language model (LLM).</p>
          <ul>
            <li>
              <a href="#ss1">Special Session 1: Techniques for Large Language Models</a>
            </li>
            <li>
              <a href="#ss2">Special Session 2: Crafting Human-Centered Chatbots: Bridging the Gaps</a>    
            </li>
          </ul>
        </div>
        <div class="card border-dark mb-3" id="ss1">
          <div class="card-header">
            <div class="row justify-content-center align-items-center">
              <h3 class="text-center">Special Session 1:<br> Techniques for Large Language Models</h3>
            </div>
          </div>
          <div class="card-body">
            <h3 class="card-title">我們與語音版 ChatGPT 的距離</h3>
            <div class="row">
              <div class="col">
                <p>
                  <b>李宏毅 Hung-Yi Lee</b><br>
                  Associate Professor, Department of Electrical Engineering and the Department in Computer Science & Information Engineering of National Taiwan University
                </p>
              </div>
            </div>
            <h4 class="card-title">Abstract</h4>
            <div class="row">
              <div class="col">
                <p class="content">在過去的幾個月裡，大型語言模型如ChatGPT的能力已經引起了大眾的廣泛討論和驚嘆，這些語言模型具有通用的處理能力，僅需給予正確的指令，往往就可以完成任務。然而，我們與語音版的ChatGPT的距離還有多遠？我們還缺少什麼才能實現語音版的ChatGPT的誕生？在這場演講中，我將分享一系列可能引領我們走向語音版ChatGPT的最新研究成果，並探討目前的挑戰、潛在的解決方案，讓我們一起攜手迎接由語音版ChatGPT帶來的未來新紀元。
                </p>
              </div>
            </div>
          
            <hr>
          
            <h3 class="card-title">Towards Human-Like Conversational AI</h3>
            <div class="row">
              <div class="col">
                <p>
                  <b>陳縕儂 Yun-Nung Chen</b><br>
                  Associate Professor, Department of Computer Science and Information Engineering in National Taiwan University
                </p>
              </div>
            </div>
            <h4 class="card-title">Abstract</h4>
            <div class="row">
              <div class="col">
                <p class="content">
                  This talk explores two crucial dimensions of Conversational AI: improved understanding and enhanced interaction. The talk will begin with a focus on refining language comprehension through the mitigation of speech recognition errors, ultimately facilitating a more comprehensive understanding of user inputs. Next, we tackle the challenge of scalability, uncovering strategies to maximize the utility of limited dialogue data, thereby enabling adaptive conversational models. Finally, we explore the realm of enriched interaction by seamlessly integrating recommendation systems into conversational interfaces. In sum, These dimensions collectively showcase the potential for substantial progress in Conversational AI, offering valuable insights for future research and innovation in the field.
                </p>
              </div>
            </div>
            <h4 class="card-title">Biography</h4>
            <div class="row">
              <div class="col">
                <p class="content">
                  Yun-Nung (Vivian) Chen is currently an associate professor in the Department of Computer Science & Information Engineering at National Taiwan University. She earned her Ph.D. degree from Carnegie Mellon University, where her research interests focus on spoken dialogue systems and natural language processing. She was recognized as the Taiwan Outstanding Young Women in Science and received Google Faculty Research Awards, Amazon AWS Machine Learning Research Awards, MOST Young Scholar Fellowship, and FAOS Young Scholar Innovation Award. Her team was selected to participate in the first Alexa Prize TaskBot Challenge in 2021. Prior to joining National Taiwan University, she worked in the Deep Learning Technology Center at Microsoft Research Redmond.
                </p>
              </div>
            </div>
          </div>
        </div>
        <div class="card border-dark mb-3" id="ss2">
          <div class="card-header">
            <div class="row justify-content-center align-items-center">
              <h3 class="text-center">Special Session 2: <br>Crafting Human-Centered Chatbots: Bridging the Gaps</h3>
            </div>
          </div>
          <div class="card-body">
            <div class="row">
              <div class="col">
                <p class="content">
                  Natural language processing (NLP) technology has advanced by leaps and bounds, and chatGPT has been widely loved by netizens since it was launched on the market since last November. Due to the shortage of manpower, the need for dialogue systems is greater than ever. However, creating a dialogue system that can interact with users in accordance with given tasks (such as syllabus, medical orders, health education) is still a considerable challenge. When an agent is given a certain task (such as teaching goal or consulting), how to guide the user to complete the goal requires more human-centered design thinking. "Crafting Human-Centered Chatbots: Bridging the Gaps" forum serves as a gathering place for professionals, researchers, and enthusiasts passionate about the development and deployment of chatbots. Here, we will explore the intricacies of designing chatbots that bridge the gaps between technology and human engagement.
                </p>
                <h4><b></b></h4>
                <p class="content"></p>
              </div>
            </div>
            <h3 class="card-title">Chair</h3>
            <div class="row">
              <div class="col">
                <p>
                  <b>張嘉惠 Chia-Hui Chang</b><br>
                  Professor, Department of Computer Science and Information Engineering in National Central University
                </p>
                <hr>
              </div>
            </div>
          </div>
          <div class="card-body">
            <h3 class="card-title">Panelists</h3>
            <h4 class="card-title">心理諮商對話系統的開發</h4>
            <div class="row">
              <div class="col">
                <p>
                  <b>簡仁宗 Jen-Tzung Chien</b><br>
                  Chair Professor, Institute of Electrical and Computer Engineering in National Yang Ming Chiao Tung University
                </p>
                <hr>
              </div>
            </div>
            <h4 class="card-title">沉浸式視覺設計互動系統</h4>
            <div class="row">
              <div class="col">
                <p>
                  <b>古倫維 Lun-Wei Ku</b><br>
                  Research Fellow/Professor, Institute of Information Science in Academia Sinica
                </p>
              </div>
            </div>
            <h4 class="card-title">教育類型對話系統的開發</h4>
            <div class="row">
              <div class="col">
                <p>
                  <b>劉晨鐘 Chen-Chung Liu</b><br>
                  Chair Professor, Department of Computer Science and Information Engineering at National Central University
                </p>
                <hr>
              </div>
            </div>
            <h4 class="card-title">從物理課的虛擬助教到人社領域的指令工程：AIGC應用於清華大學的嘗試</h4>
            <div class="row">
              <div class="col">
                <p>
                  <b>王道維 Daw-Wei Wang</b><br>
                  Director of Counselling Center, and Vice Director of Center for Application and Development of AI in HSS<br>
                  Professor in Physics Department, National Tsing-Hua University
                </p>
                <hr>
              </div>
            </div>
        </div>
      </div>
    </section>

    <!--==========================
      Shared Task Section
    ============================-->
    <section id="task" class="section-with-bg fadeInUp">
      <div class="container nodata">
        <div class="section-header">
          <h2>Shared Tasks</h2>
          <p>ROCLING 2024 will feature two shared tasks that provide an opportunity for participants to showcase their expertise and innovative approaches in tackling specific challenges. Let's take a closer look at each shared task:</p>
          <ul>
            <li>
              <a href="#st1">Shared Task 1: Chinese Multi-genre Named Entity Recognition in the Healthcare Domain (MultiNER-Health)</a>
            </li>
            <li>
              <a href="#st2">Shared Task 2: Formosa Speech Recognition Challenge 2024 (Hakka ASR)</a>    
            </li>
          </ul>
        </div>
        <div class="card border-dark mb-3" id="st1">
          <div class="card-header">
            <div class="row justify-content-center align-items-center">
              <h3 class="text-center">Shared Task 1: MultiNER-Health<br>Chinese Multi-genre Named Entity Recognition in the Healthcare Domain</h3>
            </div>
          </div>
          <div class="card-body">
            <h3 class="card-title">Organizers</h3>
            <div class="row">
              <div class="col">
                <p>
                  <b>李龍豪 Lung-Hao Lee</b><br>
                  國立中央大學電機工程學系<br>
                  Department of Electrical Engineering in National Central University<br>
                  <a href="mailto:lhlee@ee.ncu.edu.tw" target="_blank">lhlee@ee.ncu.edu.tw</a>
                </p>
                <p>
                  <b>林孜彌 Tzu-Mi Lin</b><br>
                  國立中央大學電機工程學系<br>
                  Department of Electrical Engineering in National Central University<br>
                  <a href="mailto:110521087@cc.ncu.edu.tw" target="_blank">110521087@cc.ncu.edu.tw</a>
                </p>
                <p>
                  <b>陳昭沂 Chao-Yi Chen </b><br>
                  國立中央大學電機工程學系<br>
                  Department of Electrical Engineering in National Central University<br>
                  <a href="mailto:110581007@cc.ncu.edu.tw" target="_blank">110581007@cc.ncu.edu.tw</a>
                </p>
              </div>
            </div>
            <h3 class="card-title">Registration</h3>
            <div class="row">
              <div class="col">
                <p>Please fill the registration form: <span><a href="https://forms.gle/HeUTUKM2U2NYky6Y6" target="_blank">here</a></span>. Organizers will confirm your registration and add all registrants to the google groups.</p>
              </div>
            </div>
            <h3 class="card-title">Contact</h3>
            <div class="row">
              <div class="col">
                <p>Please join our Google group for direct communication at <span><a href="mailto:rocling23-shared-task@googlegroups.com">rocling23-shared-task@googlegroups.com</a></span>.</p>
              </div>
            </div>
            <h3 class="card-title">I. Background</h3>
            <div class="row">
              <div class="col">
                <p class="content">Named Entity Recognition (NER) is a fundamental task in information extraction that locates the mentions of named entities and classifies them (e.g., person, organization and location) in unstructured texts. The NER task has traditionally been solved as a sequence labeling problem, where entity boundaries and category labels are jointly predicted. Chinese NER is more difficult to process than English NER. Chinese language is logographic and provides no conventional features like capitalization. In addition, due to a lack of delimiters between characters, Chinese NER is correlated with word segmentation, and named entity boundaries are also word boundaries. However, incorrectly segmented entity boundaries will cause error propagation in NER. For example, in a particular context, a disease entity “思覺失調症” (schizophrenia) may be incorrectly segmented into three words: “思覺” (thinking and feeling), “失調” (disorder) and “症” (disease).</p>
                <p class="content">In the digital era, healthcare information-seeking users usually search and browse web content in click-through trails to obtain healthcare-related information before making a doctor’s appointment for diagnosis and treatment. Web texts are valuable sources to provide healthcare information such as health-related news, digital health magazines and medical question/answer forums. Domain-specific healthcare information includes many proper names, mainly as named entities, such as “葡萄糖六磷酸鹽去氫酶” (Glucose-6-Phosphate Dehydrogenase; G6PD), “電腦斷層掃描” (computed tomography; CT), and “靜脈免疫球蛋白注射” (intravenous immunoglobulin; IVIG). In summary, Chinese healthcare NER is an important and essential task in natural language processing to automatically identify healthcare entities such as symptoms, chemicals, diseases, and treatments for machine reading and understanding.</p>
                <p class="content">Following the ROCLING-2022 shared task focused on Chinese healthcare NER, we organize a MultiNER-Health shared task for multi-genre NER in the healthcare domain. In this shared task, we have three genres:
                  <ol>
                    <li>Formal texts (FT): this includes health news and articles written by professional editors or journalists.</li>
                    <li>Social media (SM): this contains texts from crowed users in medical question/answer forums.</li>
                    <li>Wikipedia articles (WA): this free online encyclopedia includes articles created and edited by volunteers worldwide</li>
                  </ol>                  
                </p>
                <p class="content">Named entities may be used in different word forms in other genres. For example, “後天免疫缺乏症候群” (Acquired Immunodeficiency Syndrome; AIDS) is commonly used as a spoken language form “愛滋病” in the medical forums. On the other hand, “甘油三酯” is a different usage referred to as “三酸甘油酯” (triglyceride; TG) in Wikipedia.</p>
              </div>
            </div>
            <h3 class="card-title">II. Task Description</h3>
            <div class="row">
              <div class="col">
                <p class="content">A total of 10 entity types are described and some examples are provided in Table I for Chinese healthcare NER. In this task, participants are asked to predict the named entity boundaries and categories for each given sentence. We use the common BIO (Beginning, Inside, and Outside) format for NER tasks. The B-prefix before a tag indicates that the character is the beginning of a named entity and I-prefix before a tag indicates that the character is inside a named entity. An O tag indicates that a token belongs to no named entity. Below are the example sentences.</p>
                <p class="table-title">Table 1. Named Entity Types</p>
                <table class="table table-responsive-lg">
                  <tbody>
                    <tr>
                      <td class="entity-type"><b>Entity Type</b></td>
                      <td><b>Description</b></td>
                      <td><b>Examples</b></td>
                    </tr>
                    <tr>
                      <td class="entity-type">Body<br>(BODY)</td>
                      <td>The whole physical structure that forms a person or animal including biological cells, organizations, organs and systems.</td>
                      <td>“細胞核” (nucleus), “神經組織” (nerve tissue), “左心房” (left atrium),  “脊髓” (spinal cord), “呼吸系統” (respiratory system)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Symptom<br>(SYMP)</td>
                      <td>Any feeling of illness or physical or mental change that is caused by a particular disease.</td>
                      <td>“流鼻水” (rhinorrhea), “咳嗽” (cough), “貧血” (anemia), “失眠” (insomnia), “心悸” (palpitation), “耳鳴” (tinnitus)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Instrument<br>(INST)</td>
                      <td>A tool or other device used for performing a particular medical task such as diagnosis and treatments.</td>
                      <td>“血壓計” (blood pressure meter), “達文西手臂” (DaVinci Robots), “體脂肪計” (body fat monitor), “雷射手術刀” (laser scalpel)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Examination<br>(EXAM)</td>
                      <td>The act of looking at or checking something carefully in order to discover possible diseases.</td>
                      <td>“聽力檢查” (hearing test), “腦電波圖” (electroencephalography; EEG),  “核磁共振造影” (magnetic resonance imaging; MRI)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Chemical<br>(CHEM)</td>
                      <td>Any basic chemical element typically found in the human body.</td>
                      <td>“去氧核糖核酸” (deoxyribonucleic acid; DNA), “糖化血色素” (glycated hemoglobin), “膽固醇” (cholesterol), “尿酸” (uric acid)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Disease<br>(DISE)</td>
                      <td>An illness of people or animals caused by infection or a failure of health rather than by an accident.</td>
                      <td>“小兒麻痺症” (poliomyelitis; polio), “帕金森氏症” (Parkinson’s disease), “青光眼” (glaucoma), “肺結核” (tuberculosis)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Drug<br>(DRUG)</td>
                      <td>Any natural or artificially made chemical used as a medicine.</td>
                      <td>“阿斯匹靈” (aspirin), “普拿疼” (acetaminophen), “青黴素” (penicillin), “流感疫苗” (influenza vaccination)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Supplement<br>(SUPP)</td>
                      <td>Something added to something else to improve human health.</td>
                      <td>“維他命” (vitamin), “膠原蛋白” (collagen), “益生菌” (probiotics), “葡萄糖胺” (glucosamine), “葉黃素” (lutein)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Treatment<br>(TREAT)</td>
                      <td>A method of behavior used to treat diseases.</td>
                      <td>“藥物治療” (pharmacotherapy), “胃切除術” (gastrectomy), “標靶治療” (targeted therapy), “外科手術” (surgery)</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Time<br>(TIME)</td>
                      <td>Element of existence measured in minutes, days, years.</td>
                      <td>“嬰兒期” (infancy), “幼兒時期” (early childhood), “青春期” (adolescence), “生理期” (on one’s period), “孕期” (pregnancy)</td>
                    </tr>
                  </tbody>
                </table>
                <p class="table-title">Table 2. Shared Task Examples</p>
                <table class="table table-responsive-lg">
                  <tbody>
                    <tr>
                      <td class="entity-type"><b>Genre</b></td>
                      <td><b>Examples</b></td>
                      <td><b>Input & Output</b></td>
                    </tr>
                    <tr>
                      <td class="entity-type" rowspan="2">Formal<br>Texts</td>
                      <td class="entity-type">Ex 1</td>
                      <td>Input: 早起也能預防<span class="text-highligh">老化</span>，甚至降低<span class="text-highligh">阿茲海默症</span>的風險<br>Output: O, O, O, O, O, O, B-SYMP, I-SYMP, O, O, O, O, O, B-DISE, I-DISE, I-DISE, I-DISE, I-DISE, O, O, O</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Ex 2</td>
                      <td>Input: <span class="text-highligh">壓力</span>、<span class="text-highligh">月經</span>引起的<span class="text-highligh">痘痘</span>患者<br>Output: B-SYMP, I-SYMP, O, B-TIME, I-TIME, O, O, O, B-DISE, I-DISE, O, O</td>
                    </tr>
                    <tr>
                      <td class="entity-type" rowspan="2">Social<br>Media</td>
                      <td class="entity-type">Ex 3</td>
                      <td>Input: 如何治療<span class="text-highligh">胃食道逆流症</span>？<br>Output: O, O, O, O, B-DISE, I-DISE, I-DISE, I-DISE, I-DISE, I-DISE, O</td>
                    </tr>
                    <tr>
                      <td class="entity-type">Ex 4</td>
                      <td>Input: 請問長期打<span class="text-highligh">善思達針劑</span>是不是會<span class="text-highligh">變胖</span>?<br>Output: O, O, O, O, O, B-DRUG, I-DRUG, I-DRUG, I-DRUG, I-DRUG, O, O, O, O, B-SYMP, I-SYMP, O?</td>
                    </tr>
                    <tr>
                      <td class="entity-type" rowspan="2">Wikipedia<br>Articles</td>
                      <td class="entity-type">Ex 5</td>
                      <td>Input: <span class="text-highligh">抗生素</span>和<span class="text-highligh">維生素Ａ酸</span>可用於口服治療<span class="text-highligh">痤瘡</span><br>Output: B-DRUG, I-DRUG, I-DRUG, O, B-DRUG, I-DRUG, I-DRUG, I-DRUG, B-DRUG, O, O, O, O, O, O, O, B-DISE, I-DISE<br><span class="text-muted">(“痤瘡” is a formal usage of “痘痘” in the example 2 )</span></td>
                    </tr>
                    <tr>
                      <td class="entity-type">Ex 6</td>
                      <td>Input: <span class="text-highligh">抑酸劑</span>，又稱<span class="text-highligh">抗酸劑</span>，抑制<span class="text-highligh">胃酸</span>分泌，緩解<span class="text-highligh">燒心</span>。<br>Output: B-CHEM, I-CHEM, I-CHEM, O, O, O, B-CHEM, I-CHEM, I-CHEM, O, O, O, B-CHEM, I-CHEM, O, O, O, O, O, B-DISE, I-DISE, O<br><span class="text-muted">(“燒心” is the spoken language of “胃食道逆流症” in the example 3 )</span></td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
            <h3 class="card-title">III. Data</h3>
            <div class="row">
              <div class="col">
                <b>Training Set</b>
                <ol>
                  <li><a href="https://github.com/NCUEE-NLPLab/Chinese-HealthNER-Corpus" target="_blank">Chinese HealthNER Corpus</a></li>
                  <li><a href="https://github.com/NCUEE-NLPLab/ROCLING-2022-ST-CHNER" target="_blank">ROCLING-2022 CHNER Dataset</a></li>
                </ol>
                <table class="table table-responsive-lg text-center">
                  <tbody>
                    <tr>
                      <td> </td>
                      <td colspan="3"><b>Training Set</b></td>
                    </tr>
                    <tr>
                      <td class="entity-type"><b>Genre</b></td>
                      <td>Formal Texts</td>
                      <td>Social Media</td>
                      <td>Wikipedia Articles</td>
                    </tr>
                    <tr>
                      <td class="entity-type"><b>#Sentences</b></td>
                      <td>23,008</td>
                      <td>7,684</td>
                      <td>3,205</td>
                    </tr>
                    <tr>
                      <td class="entity-type"><b>#Characters</b></td>
                      <td>1,109,918</td>
                      <td>403,570</td>
                      <td>118,116</td>
                    </tr>
                    <tr>
                      <td class="entity-type"><b>#Named Entities</b></td>
                      <td>42,070</td>
                      <td>26,390</td>
                      <td>13,369</td>
                    </tr>
                    <tr>
                      <td class="entity-type"><b>Data Sets</b></td>
                      <td colspan="2">Chinese HealthNER Corpus<br>(Lee and Lu, 2021)</td>
                      <td>CHNER Dataset<br>(Lee et al., 2022)</td>
                    </tr>
                  </tbody>
                </table>
                <p class="content">Notes: The policy of this shared task is an open test. Participating systems are allowed to use other publicly available data for this shared task, but the use of other data should be specified in the final system description paper.</p>
                <b>Testing Set</b>
                <p class="content">Testing Set at least 2,000 Chinese sentences per genre will be provided for system performance evaluation.</p>
              </div>
            </div>
            <h3 class="card-title">IV. Evaluation</h3>
            <div class="row">
              <div class="col">
                <p class="content">The performance is evaluated by examining the difference between machine-predicted labels and human-annotated labels. We adopt standard precision, recall, and F1-score, which are the most typical evaluation metrics of NER systems at a character level. If the predicted tag of a character in terms of BIO format was completely identical with the gold standard, that is one of the defined BIO tags, the character in the testing instance was regarded as correctly recognized. Precision is defined as the percentage of named entities found by the NER system that are correct. Recall is the percentage of named entities present in the test set found by the NER system. Different genre will be evaluated independently. The <span class="text-highligh">Macro-averaging F1</span> score among three genres will be used for final ranking in the leaderboard.</p>
              </div>
            </div>
            <h3 class="card-title">V. Results</h3>
            <div class="row">
              <div class="col">
                <table class="table table-responsive-lg">
                  <tbody class="entity-type">
                    <tr class="border-top-bold">
                      <td rowspan=2><b>Team</b></td>
                      <td rowspan=2><b>Run#</b></td>
                      <td colspan=4><b>F1-score (%)</b></td>
                      <td rowspan=2><b>Rank</b></td>
                    </tr>
                    <tr class="border-bottom-bold">
                      <td><b>Formal Texts</b></td>
                      <td><b>Social Media</b></td>
                      <td><b>Wikipedia Articles</b></td>
                      <td><b>Macro-averaging</b></td>
                    </tr>
                    <tr>
                      <td>CrowNER [1]</td>
                      <td>Run 2</td>
                      <td><b>65.49</b></td>
                      <td>69.54</td>
                      <td><b>73.63</b></td>
                      <td><b>69.55</b></td>
                      <td>1</td>
                    </tr>
                    <tr>
                      <td>YNU-HPCC [2]</td>
                      <td>Run 2</td>
                      <td>61.96</td>
                      <td>71.11</td>
                      <td>72.13</td>
                      <td>68.40</td>
                      <td>2</td>
                    </tr>
                    <tr>
                      <td>ISLab [3]</td>
                      <td>Run 1</td>
                      <td>62.52</td>
                      <td><b>71.42</b></td>
                      <td>71.19</td>
                      <td>68.38</td>
                      <td>3</td>
                    </tr>
                    <tr>
                      <td>SCU-MESCLab [4]</td>
                      <td>Run 1</td>
                      <td>62.51</td>
                      <td>71.33</td>
                      <td>70.57</td>
                      <td>68.14</td>
                      <td>4</td>
                    </tr>
                    <tr>
                      <td>YNU-ISE-ZXW [5]</td>
                      <td>Run 3</td>
                      <td>62.79</td>
                      <td>70.22</td>
                      <td>70.37</td>
                      <td>67.79</td>
                      <td>5</td>
                    </tr>
                    <tr class="border-bottom-double">
                      <td>LingX [6]</td>
                      <td>Run 2</td>
                      <td>51.23</td>
                      <td>59.28</td>
                      <td>60.54</td>
                      <td>57.02</td>
                      <td>6</td>
                    </tr>
                    <tr>
                      <td class="border-bottom-bold" rowspan=2>Baseline [7]<br>(BiLSTM-CRF)</td>
                      <td>Word2vec</td>
                      <td>60.99</td>
                      <td>67.16</td>
                      <td>67.91</td>
                      <td>65.35</td>
                      <td>-</td>
                    </tr>
                    <tr class="border-bottom-bold">
                      <td>BERT</td>
                      <td>61.08</td>
                      <td>70.77</td>
                      <td>72.54</td>
                      <td>68.13</td>
                      <td>-</td>
                    </tr>
                  </tbody>
                </table>
                <ul class="reference-list">
                  <li class="reference-item" reference-number="1">
                    Yin-Chieh Wang, Wen-Hong Wu, Feng-Yu Kuo, Han-Chun Wu, Te-Yu Chi, Te-Lun Yang, Sheh Chen, and Jyh-Shing Roger Jang. 2023. CrowNER at ROCLING 2023 MultiNER-Health Task: enhancing NER task with GPT paraphrase augmentation on sparsely labeled data.
                  </li>
                  <li class="reference-item"reference-number="2">
                    Chonglin Pang, You Zhang, and Xiaobing Zhou. YUN-HPCC at ROCLING 2023 MultiNER-Health Task: a transformer-based approach for Chinese healthcare NER.
                  </li>
                  <li class="reference-item"reference-number="3">
                    Jun-Jie Wu, Tao-Hsing Chang, and Fu-Yuan Hsu. 2023. ISLab at ROCLING 2023. MultiNER-Health Task: a three-stage NER model combining textual content and label semantics.
                  </li>
                  <li class="reference-item"reference-number="4">
                    Tzu-En Su, Ruei-Cyuan Su, Ming-Hsiang Su, and Tsung-Hsien Yang. 2023. SCU-MESCLab at ROCLING 2023 MultiNER-Health Task: named entity recognition using multiple classifier model.
                  </li>
                  <li class="reference-item"reference-number="5">
                    Xingwei Zhang, Jin Wang, and Xuejie Zhang. 2023. YUN-ISE-ZXW at ROCLING 2023 MultiNER-Health Task: a transformer-based model with LoRA for Chinese healthcare named entity recognition.
                  </li>
                  <li class="reference-item"reference-number="6">
                    Xuelin Wang and Qihao Yang. 2023. LingX at ROCLING 2023 MultiNER-Health Task: intelligent capture of Chinese medical named entities by LLMs.
                  </li>
                  <li class="reference-item"reference-number="7">
                    Lung-Hao Lee, Chien-Huan Lu, and Tzu-Mi Lin. 2022. NCUEE-NLP at SemEval-2022. Task 11: Chinese named entity recognition using the BERT-BiLSTM-CRF model. In Proceedings of the 16th International Workshop on Semantic Evaluation. Association for Computational Linguistics, pages 1597-1602. 
                  </li>
                </ul>
              </div>
            </div>
            <h3 class="card-title">VI. Important Date</h3>
            <div class="row">
              <div class="col">
                <ul class="fa-ul">
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Release of training data: April 15, 2023</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Shared task registration due: July 25, 2023</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Release of test data: August 1, 2023</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Testing results submission due: August 3, 2023</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Release of evaluation results: August 7, 2023</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>System description paper due: August 25, 2023</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Notification of acceptance: September 8, 2023 </li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Camera-ready deadline: September 15, 2023</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>Main conference: October 20-21, 2023</li>
                </ul>
              </div>
            </div>
            <h3 class="card-title">VII. Baseline System</h3>
            <div class="row">
              <div class="col">
                <p class="content">We provide a starter kit on our GitHub Repo. It can be used to create the BiLSTM-CRF system for the NER tasks.<br><a href="https://github.com/NCUEE-NLPLab/AITutorial-2022-ChineseNER " target="_blank">https://github.com/NCUEE-NLPLab/AITutorial-2022-ChineseNER</a></p>
              </div>
            </div>
            <h3 class="card-title">References</h3>
            <div class="row">
              <div class="col">
                <p>
                  Lung-Hao Lee, and Yi Lu (2021). Multiple Embeddings Enhanced Multi-Graph Neural Networks for Chinese Healthcare Named Entity Recognition. IEEE Journal of Biomedical and Health Informatics (IEEE JBHI), 25(7): 2801- 2810.
                  <br><br>
                  Lung-Hao Lee, Chao-Yi Chen, Liang-Chih Yu, and Yuen-Hsien Tseng (2022). Overview of the ROCLING 2022 Shared Task for Chinese Healthcare Named Entity Recognition. In Proceedings of the 34th Conference on Computational Linguistics and Speech Processing (ROCLING'22), pp. 363-368.
                </p>
              </div>
            </div>
          </div>
        </div>
        <hr class="my-4">
        <div class="card border-dark mb-3" id='st2'>
          <div class="card-header">
            <div class="row justify-content-center align-items-center">
              <h3 class="text-center">Shared Task 2:<br>Formosa Speech Recognition Challenge 2023 (Hakka ASR)
                <br>
                2023客語語音辨認競賽
              </h3>
              <div class="small">
                <ul class="nav nav-tabs" id="st2-tab" role="tablist">
                  <li class="nav-item" role="presentation">
                    <button class="nav-link active" id="st2-english-tab" data-toggle="pill" data-target="#st2-english" type="button" role="tab" aria-controls="st2-english" aria-selected="true">English</button>
                  </li>
                  <li class="nav-item" role="presentation">
                    <button class="nav-link" id="st2-chinese-tab" data-toggle="pill" data-target="#st2-chinese" type="button" role="tab" aria-controls="st2-chinese" aria-selected="false">中文</button>
                  </li>
                </ul>
              </div>
            </div>
          </div>
          <div class="tab-content" id="st2-tabContent">
            <div class="tab-pane fade show active" id="st2-english" role="tabpanel" aria-labelledby="st2-english-tab">
              <div class="card-body">
                <h3 class="card-title">Organizers</h3>
                <div class="row">
                  <div class="col">
                    <p>
                      <b>Hakka Affairs Council</b><br>
                      <b>Industry Academia Innovation School, National Yang Ming Chiao Tung University</b>
                    </p>
                  </div>
                </div>
                <h3 class="card-title">Co-Organizers</h3>
                <div class="row">
                  <div class="col">
                    <p>
                      <b>The Association for Computational Linguistics and Chinese Language Processing</b><br>
                      <b>National Center for High-Performance Computing</b>
                    </p>
                  </div>
                </div>
                <h3 class="card-title">I. Background of the Competition Event</h3>   
                <div class="row">
                  <div class="col">
                    <p class="content">
                      In order to sustainably inherit the Hakka language, accelerate research and development in Hakka speech recognition, and promote the Taiwan Hakka Speech Database established by our organization, we are organizing the "2023 Hakka Speech Recognition Competition" to provide the existing Taiwan Hakka Speech Database as a basis for training speech recognition models for competition participants. Through this series of competitions, we aim to expedite domestic research and development in Hakka speech recognition and promote academic research and industrial development in Hakka speech AI.
                    </p>
                    <p class="content">
                      The competition is organized by Hakka Affairs Council and Industry Academia Innovation School, National Yang Ming Chiao Tung University. It is co-organized by The Association for Computational Linguistics and Chinese Language Processing and National Center for High-Performance Computing. Based on Hakka Affairs Council's speech database, we will provide training data for speech recognition models to competition participants, with the goal of injecting technological energy into the practical use of Hakka in daily life, promoting Hakka language and culture, and enhancing the popularization of Hakka usage.
                    </p>
                  </div>
                </div>
                <h3 class="card-title">II. Event Schedule and Competition Registration</h3>
                <div class="row">
                  <div class="col">
                    <table class="table table-responsive-lg">
                      <tbody>
                        <tr>
                          <td class="entity-type"><b>Date</b></td>
                          <td class="entity-type"><b>Event</b></td>
                        </tr>
                        <tr>
                          <td>June 5th to July 31st, 2023</td>
                          <td>Registration period</td>
                        </tr>
                        <tr>
                          <td>August 7th, 2023</td>
                          <td>Warm-up round (non-scoring)</td>
                        </tr>
                        <tr>
                          <td>September 11th, 2023</td>
                          <td>Final round</td>
                        </tr>
                        <tr>
                          <td>October 20th, 2023</td>
                          <td>Award ceremony and certificate presentation (physical event)</td>
                        </tr>
                        <tr>
                          <td>October 20th to 21st, 2023</td>
                          <td>Presentation of results at ROCLING 2023 Conference</td>
                        </tr>
                      </tbody>
                    </table>
                    <p>
                      <ol>
                        <li>The competition is open to all and registration is free of charge.</li>
                        <li>Registration will begin on June 5th, 2023, and end on July 31st, 2023 (please refer to the third point below for the required documents for registration). There are two categories for competition: the General Category (for members of the public) and the Student Category. Each team can have a maximum of 5 members, or individuals can register on their own. Each person can only participate in one team and cannot be part of multiple teams. It is required to designate one team member as the main contact person for communication.</li>
                        <li>In order to participate in the competition and obtain the authorization to use the "Taiwan Hakka Speech Database" provided by the Hakka Affairs Council, the following requirements must be fulfilled:
                          <ol>
                            <li style="list-style-type: lower-roman">Completion and signing of the <a href="https://rocling2023.github.io/assets/SharedTaskII-HakkaASR-IP_Protection_and_Confidentiality_Consent_Agreement.docx">"Intellectual Property Protection and Confidentiality Consent Agreement for the Taiwan Hakka Speech Database"</a> issued by the Hakka Affairs Council. The signed agreement should be sent via email to the designated team email address: SARC@nycu.edu.tw. The email should include the team name, name of the main contact person, and contact information. Once the data verification is completed by the organizing committee and confirmed to be correct, a notification email confirming successful registration will be sent. Upon receipt of the confirmation email, the registration process will be considered complete, and the email will also provide a password to download the relevant data from the website.</li>
                            <li style="list-style-type: lower-roman">Submission of the recognition results, corresponding scores, and a description of the recognition system for both the warm-up round and the final round test audio files. These results should be submitted via email to the designated team email address: SARC@nycu.edu.tw. The deadline for submission is August 14th and September 22nd respectively. Additionally, a preliminary draft of the paper must be submitted by September 22nd.</li>
                            <li style="list-style-type: lower-roman">Completion of paper revisions and submission to the ROCLING 2023 Conference by October 6th.</li>
                            <li style="list-style-type: lower-roman">The authorized use of the Hakka speech data is strictly limited to academic research and technical development and cannot be transferred to third parties or used for commercial purposes. If the above requirements cannot be fulfilled, all Hakka speech data files and related materials from the "Taiwan Hakka Speech Database" must be immediately and completely deleted.</li>
                          </ol>
                        </li>
                        <li>The registration period starts on June 5th and ends on July 31st, with the release of 60 hours of Hakka speech data files (including audio files and transcripts) from the Taiwan Hakka Speech Database. Participating teams are required to use the provided corpora and recognition module from the organizers to train and develop their own speech recognizer.</li>
                        <li>On August 7th, there will be a warm-up round:
                          <ol>
                            <li style="list-style-type: lower-roman;">There will be two competition categories: □ transcription to Hakka Chinese characters □ transcription to Hakka Pinyin. Participants can choose at least one category, and they can also participate in both categories simultaneously. The results of the recognition should be submitted before August 14th.</li>
                            <li style="list-style-type: lower-roman;">During the warm-up round, 10 hours of Hakka speech data files (audio files only) from the Taiwan Hakka Speech Database will be released.</li>
                            <li style="list-style-type: lower-roman;">The organizers will announce the answers and participants' submissions on the competition website before August 21st. </li>
                            <li style="list-style-type: lower-roman;">Please follow the specified format for submission: Use "Organization + Team Name + Participant" as the filename, and the answer should be in the format of ID Answer (with one column for the audio file ID and one column for the output of the speech recognizer)</li>
                          </ol>
                        </li>
                        <li>On September 11th, the final round of the competition will take place:
                          <ol>
                            <li style="list-style-type: lower-roman;">There will be two competition categories: □ transcription to Hakka Chinese characters □ transcription to Hakka Pinyin. Participants can choose at least one category, and they can also participate in both categories simultaneously. The results of the recognition should be submitted along with the preliminary draft of the paper before September 22nd.</li>
                            <li style="list-style-type: lower-roman;">During the final round, 10 hours of Hakka speech data files (audio files only) from the Taiwan Hakka Speech Database will be released.</li>
                            <li style="list-style-type: lower-roman;">The organizers will evaluate the submitted data and announce the answers and participants' submissions on the competition website before September 29th.</li>
                            <li style="list-style-type: lower-roman;">Please follow the specified format for submission: Use "Organization + Team Name + Participant" as the filename, and the answer should be in the format of ID Answer (with one column for the audio file ID and one column for the output of the speech recognizer).</li>
                            <li style="list-style-type: lower-roman;">Participants are required to complete the revision of their papers and submit them to the ROCLING 2023 Conference before October 6th.</li>
                          </ol>
                        </li>
                        <li><b>The organizers reserve the right to make adjustments to the content and schedule of the competition. Any updates or changes will be announced primarily on the competition website.</b></li>
                      </ol>
                    </p>
                  </div>
                </div>
                <h3 class="card-title">III. The evaluation process for the competition is as follows:</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li>Submission of Materials: Each participating team is required to submit the recognition results of the test audio files, description of the recognition system, and the paper report within the given deadline.</li>
                      <li>Since only audio files are provided for the warm-up and final rounds, the evaluation will be based on the recognition results submitted by the participating teams. A judging panel, consisting of members from the research team, will use the provided transcripts of the audio files as the reference answers and calculate the error rates according to the following systems.</li>
                      <li>The evaluation methods for the two competition categories are as follows:
                        <ol>
                          <li style="list-style-type: lower-roman;">Track 1: Transcription to Hakka Chinese characters - Calculate the Character Error Rate (CER).</li>
                          <li style="list-style-type: lower-roman;">Track 2: Transcription to Hakka Pinyin - Calculate the Syllable Error Rate (SER).</li>
                        </ol>
                      </li>
                      <li>In the final round, the ranking will be determined separately for each competition category based on the recognition rates to determine the highest-performing participants.</li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">IV. Awards and Event Achievements</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li>Certificates will be awarded to the participating teams upon completion of the competition. Specifically, certificates will be presented for the following four categories: Student Group - Hakka Characters, Student Group - Hakka Pinyin, General Group - Hakka Characters, and General Group - Hakka Pinyin. Each category will have a first-place certificate awarded (depending on the actual number of participating teams, consideration may be given to awarding second and third-place certificates). The certificate presentation ceremony is scheduled for October 20th, 112th year.</li>
                      <li>From October 20th to 21st, ROCLING 2023 will feature presentations of relevant research achievements.</li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">V. Participation Notice</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li>By participating in the competition, the teams are considered to agree to the rules and regulations of the event. In case of any disputes, the organizer reserves the right of final interpretation. Any disputes will be handled in accordance with the laws of the Republic of China, and the Taipei District Court shall have jurisdiction as the court of first instance.</li>
                      <li>Registration and Personal Rights:
                        <ol>
                          <li style="list-style-type: lower-roman;">The information provided during registration must be accurate and must not involve the impersonation or theft of anyone's data. If there is any false or incorrect information, the organizer may cancel the team's participation and eligibility for winning. If any damage is caused to the organizer or any other person's rights, all team members shall bear the relevant legal responsibilities.</li>
                          <li style="list-style-type: lower-roman;">Collection of Participants' Personal Information:
                            <ol>
                              <li style="list-style-type: square;">The organizer and co-organizer will conduct the Hakka speech recognition competition and obtain personal information from participants/teams that is submitted in the "Intellectual Property Protection and Confidentiality Consent Form for the Establishment of the Taiwan Hakka Speech Database" or other personal information that can be directly or indirectly identified.</li>
                              <li style="list-style-type: square;">Participants/teams consent to the retention of the above-mentioned personal information by the organizer for the management needs of the competition (such as system operation management, notification and contact, award certificates, event information dissemination, relevant statistical analysis, etc.).</li>
                            </ol>
                          </li>
                        </ol>
                      </li>
                      <li>Other considerations:<br>If there are any matters not covered in these event regulations, the organizer reserves the right to modify and supplement any changes, updates, or modifications to the event, in accordance with relevant legal provisions. The official announcements on the event website shall serve as the basis for such modifications and updates.</li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">VI. Contact Information for this Competition:</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li style="list-style-type: square;">Ms. Yang: 03-5712121#54554 / <a href="mailto:m31221123@nycu.edu.tw">m31221123@nycu.edu.tw</a></li>
                      <li style="list-style-type: square;">Ms. Bai: 03-5712121#54555 / <a href="mailto:pehsimju@nycu.edu.tw">pehsimju@nycu.edu.tw</a></li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">VII. The organizer reserves the right to cancel, terminate, modify, or suspend this event at any time.</h3>
              </div>
            </div>
            <div class="tab-pane fade" id="st2-chinese" role="tabpanel" aria-labelledby="st2-chinese-tab">
              <div class="card-body">
                <h3 class="card-title">主辦單位</h3>
                <div class="row">
                  <div class="col">
                    <p>
                      <b>客家委員會</b><br>
                      <b>國立陽明交通大學產學創新研究學院</b>
                    </p>
                  </div>
                </div>
                <h3 class="card-title">協辦單位</h3>
                <div class="row">
                  <div class="col">
                    <p>
                      <b>社團法人中華民國計算語言學學會</b><br>
                      <b>財團法人國家實驗研究院國家高速網路與計算中心</b>
                    </p>
                  </div>
                </div>
                <h3 class="card-title">一、 競賽活動緣起</h3>   
                <div class="row">
                  <div class="col">
                    <p class="content">
                      為永續傳承客家語言，加速客語語音辨識研究及發展，並推廣本會建置之臺灣客語語音資料庫，讓更多團隊投入客語語音開發，特辦理「2023客語語音辨認競賽活動」，規劃以現有臺灣客語語音資料庫之語音資料為基礎，提供予競賽學員進行語音辨識模型之訓練。透過此系列競賽加速國內客語語音辨識研究與產業開發進程，推動客語語音AI的學術研究與產業開發。
                    </p>
                    <p class="content">
                      本次客語語音辨認競賽主辦單位為客家委員會與國立陽明交通大學產學創新研究學院，協辦單位為社團法人中華民國計算語言學學會及財團法人國家實驗研究院國家高速網路與計算中心。擬以客委會語音資料庫為基礎，提供予競賽學員進行語音辨識模型之訓練，期許能透過本活動讓客語在生活使用中注入科技能量，推廣客家語言文化並提升客語使用之普及性。
                    </p>
                  </div>
                </div>
                <h3 class="card-title">二、 活動時程及競賽報名辦法</h3>
                <div class="row">
                  <div class="col">
                    <table class="table table-responsive-lg">
                      <tbody>
                        <tr>
                          <td class="entity-type"><b>時間</b></td>
                          <td class="entity-type"><b>活動內容</b></td>
                        </tr>
                        <tr>
                          <td>112年6月5日至7月31日</td>
                          <td>報名期間</td>
                        </tr>
                        <tr>
                          <td>112年8月7日</td>
                          <td>熱身賽(不計算成績)</td>
                        </tr>
                        <tr>
                          <td>112年9月11日</td>
                          <td>決賽</td>
                        </tr>
                        <tr>
                          <td>112年10月20日</td>
                          <td>頒獎典禮頒發獎狀(實體活動)</td>
                        </tr>
                        <tr>
                          <td>112年10月20至21日</td>
                          <td>ROCLING 2023研討會上發表成果</td>
                        </tr>
                      </tbody>
                    </table>
                    <p>
                      <ol>
                        <li>本競賽不限資格，免費報名</li>
                        <li>112年6月5日開始報名至7月31日截止(報名應備文件詳見以下第三點)，分為一般組(社會人士)及學生組競賽，每隊上限5人或個人報名皆可，每人僅可參加1個隊伍，無法重複組隊，並須推派1位團隊主要聯絡人擔任窗口聯繫。</li>
                        <li>報名競賽需全程參與本活動，始能獲得「客家委員會」授權「臺灣客語語音資料庫」之客語語音語料音檔，詳述如下：
                          <ol>
                            <li style="list-style-type: lower-roman">須完成<a href="https://rocling2023.github.io/assets/SharedTaskII-HakkaASR-IP_Protection_and_Confidentiality_Consent_Agreement.docx">「客家委員會建置臺灣客語語音資料庫智慧財產保護暨保密同意書」</a>簽署，並於信件中署名隊伍名稱、主要聯絡人姓名及聯繫方式寄回工作小組信箱SARC@nycu.edu.tw，待工作小組完成資料驗證並確認無誤後將會寄回報名成功通知信件，收到確認信件後始完成報名程序，信件內將一併提供密碼可至網站下載相關資料。</li>
                            <li style="list-style-type: lower-roman">須分別於112年8月14日及9月22日前繳回熱身賽及決賽測試音檔的辨認結果、與結果相對應之辨認分數、辨認系統描述說明至工作小組信箱SARC@nycu.edu.tw(9月22日前亦須一併繳交論文初稿)。</li>
                            <li style="list-style-type: lower-roman">須於10月6日前完成論文修改並投稿至ROCLING 2023研討會。</li>
                            <li style="list-style-type: lower-roman"><b>本活動授權客語語音語料不得轉授權予第三人且僅供學術研究及技術開發，禁止用於商業用途。倘無法完成上述要件，須即時完整刪除所有「臺灣客語語音資料庫」之客語語音語料音檔及相關資料。</b></li>
                          </ol>
                        </li>
                        <li>112年6月5日至7月31日開始報名&釋放臺灣客語語音資料庫之客語語音語料音檔60小時(含音檔及文字稿)：參賽隊伍需透過主辦單位提供的語料及辨識模組訓練並完成自己的語音辨認器。</li>
                        <li>112年8月7日熱身賽：:
                          <ol>
                            <li style="list-style-type: lower-roman;">2個競賽項目分別為將音檔轉出□客語漢字、□客語拼音(至少擇1項，亦可同時參加)，並於8月14日前繳交其辨認結果。</li>
                            <li style="list-style-type: lower-roman;">釋放臺灣客語語音資料庫之客語語音語料音檔10小時 (僅釋放音檔)。</li>
                            <li style="list-style-type: lower-roman;">主辦單位將於8月21日前公告答案及參賽隊伍繳交成果於競賽官網中(不計算成績)。</li>
                            <li style="list-style-type: lower-roman;">繳交格式：檔名請以「單位＋隊名＋參賽者」為檔名；答案格式：ID 答案（一欄為音檔ID，一欄為語音辨認器輸出）。</li>
                          </ol>
                        </li>
                        <li>112年9月11日決賽： 
                          <ol>
                            <li style="list-style-type: lower-roman;">2個競賽項目分別為將音檔轉出□客語漢字、□客語拼音，(至少擇1項，亦可同時參加)，並於9月22日前繳交其辨認結果及論文初稿。</li>
                            <li style="list-style-type: lower-roman;">釋放臺灣客語語音資料庫之客語語音語料音檔10小時(僅釋放音檔)。</li>
                            <li style="list-style-type: lower-roman;">主辦單位就回收之資料進行評分並於9月29日前公告答案及參賽隊伍繳交成果於競賽官網中。</li>
                            <li style="list-style-type: lower-roman;">繳交格式：檔名請以「單位＋隊名＋參賽者」為檔名；答案格式：ID 答案（一欄為音檔ID，一欄為語音辨認器輸出）。</li>
                            <li style="list-style-type: lower-roman;">於10月6日前完成論文修改並投稿至ROCLING 2023研討會。</li>
                          </ol>
                        </li>
                        <li><b>主辦單位保有調整內容及時間之權利，相關內容以網站公告為主</b></li>
                      </ol>
                    </p>
                  </div>
                </div>
                <h3 class="card-title">三、 評分方式</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li>交付資料：各參賽隊伍須在期限前繳交測試音檔的辨認結果、辨認系統描述說明及論文報告。</li>
                      <li>由於熱身及決賽僅釋放音檔，評分方式為參賽隊伍繳交其辨認結果，由研究團隊成員組成評審工作小組，分別針對2個競賽項目以客委會提供之音檔文字稿為標準答案並以下列系統計算錯誤率進行評分。</li>
                      <li>2個競賽項目評分方式如下：
                        <ol>
                          <li style="list-style-type: lower-roman;">Track1將音檔轉出客語漢字：計算CER（字元錯誤率）。</li>
                          <li style="list-style-type: lower-roman;">Track2將音檔轉出客語拼音：計算SER（音節錯誤率）。</li>
                        </ol>
                      </li>
                      <li>決賽將分別就2個競賽項目進行該辨識率高低排序進行排名。</li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">四、 頒獎及活動成果</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li>頒發參賽隊伍完賽獎狀，並分別頒發學生組客語漢字組別、學生組客語拼音組別及一般組客語漢字組別、一般組客語拼音組別共計4個組別第一名獎狀，(屆時將視實際參賽隊伍數量考量是否頒發第二、三名獎狀)，並訂於112年10月20日頒發獎狀。</li>
                      <li>112年10月20至21日ROCLING 2023中發表相關研究成果。</li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">五、 參賽須知</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li>參賽隊伍參與競賽，視同同意活動辦法及各項規定，若有任何爭議，主辦單位保有最終解釋權，任何有關之爭議，均依中華民國法律處理，並以臺灣臺北地方法院為第一審管轄法院。</li>
                      <li>報名與個人權益：
                        <ol>
                          <li style="list-style-type: lower-roman;">報名所填寫之資料必須詳實，不可冒用或盜用任何人之資料。如有不實或不正確之情事，主辦單位得取消參賽及得獎資格。如有致損害於主辦單位或其他任何人之相關權益，參賽隊伍全體隊員應自負相關法律責任。</li>
                          <li style="list-style-type: lower-roman;">蒐集參賽者個人資料告知事項：
                            <ol>
                              <li style="list-style-type: square;">主辦單位與協辦單位，辦理客語語音辨認競賽活動，獲取參賽者/參賽隊伍回傳客家委員會建置臺灣客語語音資料庫智慧財產保護暨保密同意書上之個人資料，或其他得以直接或間接識別之個人資料。</li>
                              <li style="list-style-type: square;">參賽者/參賽隊伍同意留存上述相關個人資料作為主辦單位管理競賽需要之用( 例如：系統作業管理、通知聯繫、得獎證書、活動訊息發佈、相關統計分析等使用 )。</li>
                            </ol>
                          </li>
                        </ol>
                      </li>
                      <li>其它注意事項：<br>本活動辦法如有未盡事宜，除依法律相關規定外，主辦單位保留修改及補充包括活動之任何異動、更新、修改之權利，並以本活動網站公告為依據。</li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">六、 本競賽活動聯絡窗口</h3>
                <div class="row">
                  <div class="col">
                    <ol>
                      <li style="list-style-type: square;">楊小姐: 03-5712121#54554 / <a href="mailto:m31221123@nycu.edu.tw">m31221123@nycu.edu.tw</a></li>
                      <li style="list-style-type: square;">白小姐: 03-5712121#54555 / <a href="mailto:pehsimju@nycu.edu.tw">pehsimju@nycu.edu.tw</a></li>
                    </ol>
                  </div>
                </div>
                <h3 class="card-title">七、 主辦單位保留隨時取消、終止、變更或暫停本案之權利</h3>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      AI Tutorial Section
    ============================-->
    <section id="tutorial" class="wow fadeInUp">
      <div class="container nodata">
        <div class="section-header">
          <h2>AI Tutorial</h2>
          <p>ROCLING 2023 will include two AI tutorials that offer participants new insights into the latest trends in Graph Neural Network (GNN).</p>
          <ul>
            <li>
              <a href="#ai1">AI Tutorial I: Demystifying Graph Neural Networks: Essentials, Applications, and Trends</a>
            </li>
            <li>
              <a href="#ai2">AI Tutorial II: Chaining Language and Knowledge Resources with LLM(s)</a>    
            </li>
          </ul>
        </div>

        <div class="card border-dark mb-3" id="ai1">
          <div class="card-header">
            <div class="row justify-content-center align-items-center">
              <h3 class="text-center">AI Tutorial I:<br>Demystifying Graph Neural Networks: Essentials, Applications, and Trends</h3>
            </div>
          </div>
          <div class="card-body">
            <div class="row">
              <div class="col">
                <p>
                  <b>李政德 Cheng-Te Li</b><br>
                  Professor, Institute of Data Science and Department of Statistics of National Cheng Kung University
                </p>
              </div>
            </div>
          </div>
        </div>

        <div class="card border-dark mb-3" id="ai2">
          <div class="card-header">
            <div class="row justify-content-center align-items-center">
              <h3 class="text-center">AI Tutorial II:<br>Chaining Language and Knowledge Resources with LLM(s)</h3>
            </div>
          </div>
          <div class="card-body">
            <div class="row">
              <div class="col">
                <p>
                  <b>謝舒凱 Shu-Kai Hsieh</b><br>
                  Professor, Graduate Institute of Linguistics &Graduate Institute of Brain and Mind, College of MedicineNational Taiwan University
                </p>
              </div>
            </div>
            <h4 class="card-title">Abstract</h4>
            <div class="row">
              <div class="col">
                Language and knowledge resources digitally embody human conceptual knowledge and have always played a pivotal role in natural language understanding. With the rise of large language models, the relationship between language resources and these models has emerged as a topic worthy of exploration. This tutorial will introduce the basic concepts of both and delve into their potential interconnections and applications. Moreover, from a practical standpoint, we will provide a hands-on notebook to guide participants in experiencing how to connect the two using the 'langchain' framework.
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Registration Section
    ============================-->
    <section id="buy-registrations" class="section-with-bg fadeInUp">
      <div class="container nodata">
        <div class="section-header nodata">
          <h2>Registrations</h2>
          <p class="text-muted">Time Zone: UTC/GMT +08:00 (Asia/Taipei)</p>
        </div>
        <div class="row">
          <div class="col-lg-4">
            <div class="card mb-5 mb-lg-3">
              <div class="card-body text-muted">
                <h5 class="card-title text-center">Early <br>Registrations</h5>
                <p class="text-highligh text-center">Before September 15, 2023</p>
                <hr>
                <h5 class="text-center">Regular</h5>
                <ul class="fa-ul">
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Member: NT$ 4,000</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Non-Member: NT$ 5,000</li>
                </ul>
                <br>
                <h5 class="text-center">Student</h5>
                <ul class="fa-ul">
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Member: NT$ 1,500</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Non-Member: NT$ 2,000</li>
                </ul>
                <hr>
                <div class="text-center">
                  <button type="button" class="btn" disabled>Register here</button>
                </div>
              </div>
            </div>
          </div>
          <div class="col-lg-4">
            <div class="card mb-5 mb-lg-3">
              <div class="card-body text-muted">
                <h5 class="card-title text-center">Late <br>Registrations</h5>
                <p class="text-center"> September 16 - October 10, 2023</p>
                <hr>
                <h5 class="text-center">Regular</h5>
                <ul class="fa-ul">
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Member: NT$ 4,300</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Non-Member: NT$ 5,300</li>
                </ul>
                <br>
                <h5 class="text-center">Student</h5>
                <ul class="fa-ul">
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Member: NT$ 1,800</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Non-Member: NT$ 2,300</li>
                </ul>
                <hr>
                <div class="text-center">
                  <button type="button" class="btn" onclick="window.open('https://conference.iis.sinica.edu.tw/surl/rocling2023/reg','_blank')" disabled>Register here</button>
                </div>
              </div>
            </div>
          </div>
          <div class="col-lg-4">
            <div class="card mb-5 mb-lg-3">
              <div class="card-body">
                <h5 class="card-title text-center">On-Site <br>Registrations</h5>
                <p class="text-center">October 20 - 21, 2023</p>
                <hr>
                <h5 class="text-center">Regular</h5>
                <ul class="fa-ul">
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Member: NT$ 4,500</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Non-Member: NT$ 5,500</li>
                </ul>
                <br>
                <h5 class="text-center">Student</h5>
                <ul class="fa-ul">
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Member: NT$ 2,000</li>
                  <li><span class="fa-li"><i class="fa fa-check"></i></span>ACLCLP Non-Member: NT$ 2,500</li>
                </ul>
                <hr>
                <div class="text-center">
                  <button type="button" class="btn" data-toggle="modal" data-target="#buy-registration-modal" data-registration-type="premium-access" disabled>Only on-site registrations</button>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="">
          <div class="">
            <div class="card border-dark mb-3">
              <div class="card-body">
                <h3>註冊說明</h3>
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    一般參與者且不具學生身份者需註冊 Regular Fee。一般參與者且具學生身份者可選擇註冊 Student Fee。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    每篇會議論文(包含shared tasks)的發表至少要有一位作者在早鳥註冊截止前繳交<span class="text-highligh">一般報名費</span>
                    (regular fee)。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    報名費含大會紀念品、午餐、茶點及晚宴，報名費一經繳費後恕不接受退費，會後將郵寄相關資料予報名者。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    ACLCLP Member 為「中華民國計算語言學學會」之有效會員。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    本年度尚未繳交年費之舊會員或失效之會員，與會身份/Category請勾選「….(會員+會費)」，勿再重複申請入會。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    非會員欲同時申請入會者，請先至學會網頁之「會員專區」申請加入會員；報名時「與會身份/Category」請勾選「….(會員+會費)」。<a href="http://www.aclclp.org.tw/member/index.php" target="_blank">(前往會員專區)</a>
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    以「學生新會員」及「學生非會員」身份報名者，請於報名時上傳學生身份證明。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    贊助單位敬請於<span class="text-highligh">10月6日</span>前完成報名手續。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    報名費收據將於會議當日報到時交付。
                  </li>
                </ul>
                <hr>
                <h3>Registration Details</h3>
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    A regular fee must be paid by the general participants (not a student).  Students (general participants) can choose to pay the student fee.
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Before early registration due, each publication paper (including shared tasks) must have at least one author pay a <span class="text-highligh">regular fee</span>.
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Registration fee includes: abstract booklet, lunches, coffee breaks, and banquet. Registration fees are non-refundable.
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    International registrants have to pay by credit card only (Visa or MasterCard). Receipt will be provided on-site.
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    A copy of a valid student ID must be uploaded into the system when registering as a student.
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Sponsor should be registed before <span class="text-highligh">October 10 </span>.
                  </li>
                </ul>
                <hr>
                <h3>報名及繳費期限</h3>
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Early Registration: <span class="text-highligh">9/15</span> (Fri) 以前，報名費應於 <span class="text-highligh">9/22</span> (Fri)前繳交。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Late Registration: <span class="text-highligh">9/16</span> (Fri) 至 <span class="text-highligh">10/10</span> (Tue)，報名費應於 10/13 (Fri) 前繳交(報名費加收300元)，線上刷卡繳費者需於 <span class="text-highligh">10/10</span> (Tue) 前完成繳費。
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    On-Site Registration: 10/10 (Tue) 線上報名截止，擬參加者，請至大會現場報名(報名費加收500元)。
                  </li>
                </ul>
                <hr>
                <h3>Important Dates for Registration</h3>
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Early Registration due by <span class="text-highligh">September 15</span> (Fri) . Payment must be received before <span class="text-highligh">September 22</span>.
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Registration between <span class="text-highligh">September 16</span> and <span class="text-highligh">October 10</span>. Payment must be received before <span class="text-highligh">October 13</span>.
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    The registration site will be closed on October 10. After that, please register on-site.
                  </li>
                </ul>
                <hr>

                <h3>Methods of Payment</h3>
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    郵政劃撥/Postal
                    <br>
                    戶名：中華民國計算語言學學會
                    <br>
                    帳號：19166251
                    <br>同一單位多位報名者可合併劃撥，請於劃撥通訊欄中註明「ROCLING及註冊編號或報名者姓名」
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    線上刷卡繳費/credit card on-line。
                  </li>
                </ul>
                <hr>
                <h3>For registration inquiries, please contact</h3>
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    聯絡人：何婉如 小姐（中華民國計算語言學學會/ACLCLP）
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    E-mail：<a href="mailto:aclclp@aclclp.org.tw">aclclp@aclclp.org.tw</a>
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    Phone Number: 02-27881638
                  </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Organization Section
    ============================-->
    <section id="organization" class="wow fadeInUp">
      <div class="container">

        <div class="section-header">
          <h2>Organization</h2>
        </div>


  
        <h3>Conference Chairs</h3>
        <div class="row">


<div class="row">
          <div class="col">
            
          </div>
          <div class="col">
            <div class="row">
              <div class="col py-3 d-flex align-items-center">
                <div class="image">
                  <img src="img/organization/曾淑娟.jpg">
                </div>
                <p><span class="chair"><a href="https://www.citi.sinica.edu.tw/pages/yu.tsao/index_zh.html" target="_blank">Yu Tsao</a></span><br>Shu-Chuan Tseng<br>Research Fellow, Academia Sinica</p>
              </div>
            </div>
          </div>
        </div>

          <div class="col">
            <div class="row">
              <div class="col py-3 d-flex align-items-center">
                <div class="image">
                  <img src="img/organization/曹昱.jpg">
                </div>
                <p><span class="chair"><a href="https://www.citi.sinica.edu.tw/pages/yu.tsao/index_zh.html" target="_blank">Yu Tsao</a></span><br>Research Fellow, Academia Sinica<br></p>
              </div>
            </div>
          </div>
			<div class="row">
              <div class="col py-3 d-flex align-items-center">
                <div class="image">
                  <img src="img/organization/黃瀚萱.png" style="height: 130%;">
                </div>
                <p><span class="chair"><a href="https://homepage.iis.sinica.edu.tw/pages/hhhuang/index_zh.html" target="_blank">Hen-Hsen Huang</a></span><br>Assistant Research Fellow, Academia Sinica<br></p>
              </div>
            </div>
        </div>
        <br>
        <br>

  
        <h3>Program Chair</h3>
        <div class="row">  
<div class="col py-3 d-flex align-items-center">
                <div class="image">
                  <img src="img/organization/范耀中.jpg">
                </div>
                <p><span class="chair"><a href="https://ouartz99.wixsite.com/tsenghc" target="_blank">Yao-Chung Fan</a></span><br> Associate Professor, National Chung Hsing University<br></p>
              </div>
        </div>
        <br>
        <br>

      </div>
    </section>

    <!--==========================
      Venue Section
    ============================-->
    <section id="venue" class="section-with-bg fadeInUp">
      <div class="container nochange">
        <div class="section-header">
          <h2>Conference Venue</h2>
          <p>Conference venue location info and gallery</p>
        </div>
        <div class="row">
          <div class="col-lg-6 venue-map">
            <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3613.2154131270954!2d121.54284987541813!3d25.09456843584992!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x3442ac228f70340d%3A0x9a0072f960485c6f!2sSoochow%20University!5e0!3m2!1sen!2stw!4v1681022050173!5m2!1sen!2stw" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
          </div>
          <div class="col-lg-6 venue-info venue-info-img1">
            <div class="row justify-content-center">
              <div class="col-11 col-lg-11">
                <h3></h3>
                <h3>First Academic Building, Waishuangsi Campus, Soochow University
                  <br>
                  東吳大學外雙溪校區
                </h3>
                <p>第一教學研究大樓</p>
                
                <br>
                <p>台灣台北市士林區臨溪路70號</p>
                <p>No.70, Linhsi Road, Shihlin District, Taipei City 111, Taiwan</p>
                <p><a href="https://goo.gl/maps/jFinS5wdSVN31x8TA" target="_blank">https://goo.gl/maps/jFinS5wdSVN31x8TA</a></p>

                <br>

              </div>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-6 venue-info venue-info-img2">
            <div class="row justify-content-center">
              <div class="col-11 col-lg-11">
                <h3>By City Buses</h3>
                <ul class="fa-ul text-white">
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    255, 268, 304, 620, 645, minibuses 18 and 19, red 30 —Soochow University_Ch'ien Mu House stop
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-check"></i></span>
                    557 —Soochow University stop
                  </li>
                </ul>
                <h3>By MRT</h3>
                <ul class="fa-ul text-white">
                 <li>
                   <span class="fa-li"><i class="fa fa-check"></i></span>
                   Shihlin stop, transfer buses 255, 304, 620, minibuses 18 and 19, red 30 —Soochow University stop
                 </li>
                </ul>
              </div>
            </div>
          </div>
          <div class="col-lg-6 venue-info venue-info-img3">
            <div class="row justify-content-center">
              <div class="col-11 col-lg-11">
                <h3>By Train</h3>
                <ul class="fa-ul text-white">
                 <li>
                   <span class="fa-li"><i class="fa fa-check"></i></span>
                   Taipei Station, transfer MRT to Shihlin stop, transfer buses 255, 304, 620, minibuses 18 and 19, red 30 — Soochow University stop
                 </li>
                </ul>
                <h3>By Self Driving</h3>
                <ul class="fa-ul text-white">
                 <li>
                   <span class="fa-li"><i class="fa fa-check"></i></span>
                   Sun Yat-sen Highway → Chongqing N. Rd. interchange (to Shihlin)→ Chongqing N. Rd. Sec. 4 → Bailing Bridge → Zhongzheng Rd. →Fulin Rd. → Zhishan Rd. → Waishuangsi Campus
                 </li>
                 <li>學校停車場無對外面開放，請多加利用大眾交通工具</li>
                 <li>學校周圍於至善路設有路邊公有停車格</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
        <div class="container-fluid venue-gallery-container">
          <div class="row justify-content-center">
            <div class="col-lg-6 col-md-6 text-center">
              <div class="venue-gallery">
                <img src="img/shilin.gif" alt="" class="img-fluid">
              </div>
            </div>
            <div class="col-lg-6 col-md-6 text-center">
              <div class="venue-gallery">
                <img src="img/jianan.gif" alt="" class="img-fluid">
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Sponsors Section
    ============================-->
    <section id="sponsors" class="wow fadeInUp nodata">
      <div class="container">
        <div class="section-header">
          <h2>Sponsor</h2>
        </div>
        <div class="row no-gutters sponsors-wrap clearfix">
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.nstc.gov.tw/" target="_blank">
                <img src="img/sponsors/1.jpg" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://iis.sinica.edu.tw/zh/index.html" target="_blank">
                <img src="img/sponsors/logo-iis.png" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.citi.sinica.edu.tw" target="_blank">
                <img src="img/sponsors/logo-citi.png" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.cyberon.com.tw/" target="_blank">
                <img src="img/sponsors/2.jpg" class="img-fluid" alt="">
              </a>
            </div>
          </div>
        </div>
        <div class="row no-gutters sponsors-wrap clearfix">
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.itri.org.tw/" target="_blank">
                <img src="img/sponsors/itri_CEL_A.png" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.iii.org.tw/" target="_blank">
                <img src="img/sponsors/iii_logo.png" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.cht.com.tw/home/consumer" target="_blank">
                <img src="img/sponsors/chunghwa.jpg" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="http://www.ez-ai.com.tw/" target="_blank">
                <img src="img/sponsors/ezai.jpg" class="img-fluid" alt="">
              </a>
            </div>
          </div>
        </div>
        <div class="row no-gutters sponsors-wrap clearfix">
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.eland.com.tw/" target="_blank">
                <img src="img/sponsors/eland.png" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.tmnewa.com.tw/" target="_blank">
                <img src="img/sponsors/tm_newa_combined_1_rgb_l.jpg" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="https://www.esunbank.com/zh-tw/personal" target="_blank">
                <img src="img/sponsors/esun.png" class="img-fluid" alt="">
              </a>
            </div>
          </div>
          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="sponsor-logo">
              <a href="http://ai.scu.edu.tw/" target="_blank">
                <img src="img/sponsors/AI_logo.gif" class="img-fluid" alt="">
              </a>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!--==========================
      Hotels Section
    ============================-->
    <section id="hotels" class="section-with-bg fadeInUp nodata">
      <div class="container nochange">
        <div class="section-header">
          <h2>Hotel</h2>
          <p>Here are some nearby hotels</p>
        </div>
        <div class="row">
          <div class="col-lg-4 col-md-6">
            <div class="hotel">
              <div class="hotel-img">
                <img src="img/hotels/soochow.jpg" alt="東吳大學 學人招待所" class="img-fluid">
              </div>
              <h3><a href="https://guesthouse.inv.scu.edu.tw" target="_blank">東吳大學學人招待所</a></h3>
              <div class="stars">
                <i class="fa fa-thumbs-up"></i>
                <i class="fa fa-thumbs-up"></i>
                <i class="fa fa-thumbs-up"></i>
                <i class="fa fa-thumbs-up"></i>
                <i class="fa fa-thumbs-up"></i>
              </div>
              <div class="infomation">
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-map"></i></span>No.70, Linhsi Road, Shihlin District, Taipei City 111, Taiwan<br><span class="small">台北市士林區臨溪路70號</span>
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-street-view"></i></span>0.0 km from the Venue
                  </li>
                </ul>
              </div>
            </div>
          </div>
          <div class="col-lg-4 col-md-6">
            <div class="hotel">
              <div class="hotel-img">
                <img src="img/hotels/renaissance.jpg" alt="台北士林萬麗酒店" class="img-fluid">
              </div>
              <h3><a href="https://www.gobooking.com.tw/Renaissance/ShihlinTaipei" target="_blank">Renaissance Taipei Shihlin Hotel<br><span class="small">台北士林萬麗酒店</span></a></h3>
              <div class="stars">
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
              </div>
              <div class="infomation">
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-map"></i></span>No. 8 Ln. 470, Section 5, Zhongshan North Road, Shilin District, Taipei City 111, Taiwan<br><span class="small">台北市士林區中山北路五段470巷8號</span>
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-street-view"></i></span>1.7 km from the Venue
                  </li>
                </ul>
              </div>
            </div>
          </div>
          <div class="col-lg-4 col-md-6">
            <div class="hotel">
              <div class="hotel-img">
                <img src="img/hotels/mayfull.jpeg" alt="台北美福大飯店" class="img-fluid">
              </div>
              <h3><a href="https://www.grandmayfull.com" target="_blank">Grand Mayfull Hotel Taipei<br><span class="small">台北美福大飯店</span></a></h3>
              <div class="stars">
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
                <i class="fa fa-star"></i>
              </div>
              <div class="infomation">
                <ul class="fa-ul">
                  <li>
                    <span class="fa-li"><i class="fa fa-map"></i></span>No.55, Lequn 2nd Rd., Zhongshan Dist., Taipei 10462, Taiwan<br><span class="small">台北市中山區樂群二路55號</span>
                  </li>
                  <li>
                    <span class="fa-li"><i class="fa fa-street-view"></i></span>3.3 km from the Venue
                  </li>
                </ul>
              </div>
            </div>
          </div>

        </div>
      </div>
    </section>

    <!--==========================
      F.A.Q Section
    ============================-->
    <section id="faq" class="wow fadeInUp nochange">
      <div class="container">
        <div class="section-header">
          <h2>F.A.Q</h2>
        </div>
        <div class="row justify-content-center">
          <div class="col-lg-9">
              <ul id="faq-list">
                <li>
                  <a data-toggle="collapse" class="collapsed" href="#faq1">
                    如果一次投稿多篇文章，該如何報名？
                    <span class="eng-content">How do I register if I submit multiple articles at once?</span>
                    <i class="fa fa-minus-circle"></i>
                  </a>
                  <div id="faq1" class="collapse" data-parent="#faq-list">
                    <p>
                      每一篇投稿文章都必須至少有一名作者進行 Regular Register。也就是說，若您投稿兩篇文章，則必須分別為這兩篇文章進行 Regular Register。
                    </p>
                    <p class="eng-content">
                      There must be at least one author registered under the Regular registraction for each article submitted. As an example, if you submit two articles, both must be registered under the "Regular" registraction. 
                    </p>
                  </div>
                </li>
                <li>
                  <a data-toggle="collapse" class="collapsed" href="#faq2">
                    中華民國計算語言學學會是什麼？如果我要註冊 ROCLING 2023 一定要註冊中華民國計算語言學學會嗎?
                    <span class="eng-content">What is ACLCLP Member? For ROCLING 2023, do I need to register as an ACLCLP member?</span>
                    <i class="fa fa-minus-circle"></i>
                  </a>
                  <div id="faq2" class="collapse" data-parent="#faq-list">
                    <p class="faq-content">
                      有關「中華民國計算語言學學會」之介紹與註冊請洽<a href="https://www.aclclp.org.tw/member/index.php" target="_blank">學會官網</a>。若您不是學會成員也可以註冊 ROCLING 2023，詳情請參考<a href="#buy-registrations">註冊資訊</a>。
                    </p>
                    <p class="eng-content">
                      For information and registration regarding the ACLCLP member, please visit the <a href="https://www.aclclp.org.tw/member/index.php" target="_blank">official website of the ACLCLP</a>. You can still register for ROCLING 2023 even if you aren't a member of the ACLCLP. For more details, please refer to the <a href="#buy-registrations">registration information</a>.
                    </p>
                  </div>
                </li>
                <li>
                  <a data-toggle="collapse" class="collapsed" href="#faq3">
                    我該如何繳交註冊費用？
                    <span class="eng-content">How can I pay the registration fee?</span>
                    <i class="fa fa-minus-circle"></i>
                  </a>
                  <div id="faq3" class="collapse" data-parent="#faq-list">
                    <p class="faq-content">
                      您可以使用郵政劃撥或是線上刷卡繳費，詳情請參考<a href="#buy-registrations">註冊資訊</a>。
                    </p>
                    <p class="eng-content">
                      You can pay via postal transfer or credit card online.Please refer to the <a href="#buy-registrations">registration information</a> for specific instructions.
                    </p>
                  </div>
                </li>
                <li>
                  <a data-toggle="collapse" class="collapsed" href="#faq4">
                    如果我還有疑問該詢問誰？
                    <span class="eng-content">If I have any questions, who should I contact?</span>
                    <i class="fa fa-minus-circle"></i>
                  </a>
                  <div id="faq4" class="collapse" data-parent="#faq-list">
                    <ul class="faq-content">
                      <li>註冊相關疑問，請聯絡何婉如小姐/Email: <a href="mailto:aclclp@aclclp.org.tw">aclclp@aclclp.org.tw</a>/Phone Number: 02-27881638。</li>
                      <li>Shared Task I 相關疑問，請寄信至 <a href="mailto:rocling23-shared-task@googlegroups.com">rocling23-shared-task@googlegroups.com</a>。</li>
                      <li>Shared Task II 相關疑問，請聯絡:
                        <ul>
                          <li>楊小姐/Email: <a href="mailto:m31221123@nycu.edu.tw">m31221123@nycu.edu.tw</a>/Phone Number: 03-5712121#54554</li>
                          <li>白小姐/Email: <a href="mailto:pehsimju@nycu.edu.tw">pehsimju@nycu.edu.tw</a>/Phone Number: 03-5712121#54555</li>
                        </ul>
                      </li>
                      <li>投稿與其他疑問，請寄信至 <a href="mailto:rocling2023@gmail.com">rocling2023@gmail.com</a>。</li>
                    </ul>
                    <ul class="eng-content">
                      <li>For registration inquiries, please contact Ms. Ho at Email: <a href="mailto:aclclp@aclclp.org.tw">aclclp@aclclp.org.tw</a>/Phone Number: 02-27881638.</li>
                      <li>For Shared Task I, please reach out to <a href="mailto:rocling23-shared-task@googlegroups.com">rocling23-shared-task@googlegroups.com</a>.</li>
                      <li>For Shared Task II, please contact:
                        <ul>
                          <li>Ms. Yang at Email: <a href="mailto:m31221123@nycu.edu.tw">m31221123@nycu.edu.tw</a>/Phone Number: 03-5712121#54554.</li>
                          <li>Ms. Bai at Email: <a href="mailto:pehsimju@nycu.edu.tw">pehsimju@nycu.edu.tw</a>/Phone Number: 03-5712121#54555.</li>
                        </ul>
                      </li>
                      <li>For submission and other questions, please contact to <a href="mailto:rocling2023@gmail.com">rocling2023@gmail.com</a>.</li>
                    </ul>
                  </div>
                </li>
              </ul>
          </div>
        </div>
      </div>
    </section>

  </main>


  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
    <div class="footer-top">
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-lg-4 col-md-6 footer-info">
            <img src="img/rocling-2023-logo-color.png" alt="ROCLING 2023">
            <p class="">The 35th annual Conference on Computational Linguistics and Speech Processing (ROCLING 2024) will be held in Academia Sinica, Taipei City, Taiwan from November 4th - 5th , 2024, 2024.</p>
          </div>
          <div class="col-lg-4 col-md-3 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="fa fa-angle-right"></i> <a href="https://www.sinica.edu.tw/" target="_blank">SINICA</a></li>
              <li><i class="fa fa-angle-right"></i> <a href="https://www.aclclp.org.tw" target="_blank">ACLCLP</a></li>
            </ul>
          </div>
          <div class="col-lg-4 col-md-6 footer-contact">
            <h4>Contact Us</h4>
            <p>
              111002 台北市士林區臨溪路70號 <br>
              No.70, Linhsi Road, Shihlin District, Taipei City 111, Taiwan <br>
              Email:<a href="mailto:rocling2023@gmail.com"> rocling2023@gmail.com</a><br>
            </p>
            <div class="social-links d-none">
              <a href="#" class="twitter"><i class="fa fa-twitter"></i></a>
              <a href="#" class="facebook"><i class="fa fa-facebook"></i></a>
              <a href="#" class="instagram"><i class="fa fa-instagram"></i></a>
              <a href="#" class="google-plus"><i class="fa fa-google-plus"></i></a>
              <a href="#" class="linkedin"><i class="fa fa-linkedin"></i></a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="copyright">
        &copy; Copyright <script>document.write(new Date().getFullYear());</script> <a href="https://nlp.bigdata.scu.edu.tw"><strong>SCU NLP LAB</strong></a>, All Rights Reserved.
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=TheEvent
        -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
        Photo by <a href="https://unsplash.com/@louis_cheng?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Louis Cheng</a> on <a href="https://unsplash.com/s/photos/taipei?orientation=landscape&utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>

  <script>
  const imageUrls = ['2T1A0930.jpg', '2T1A1200.jpg', '2T1A1301.jpg', 
                     '2T1A1556.jpg', '2T1A2105.jpg', 'DSC02088.jpg', 'DSC02566.jpg', 
                     'DSC02693.jpg', '2T1A0591.jpg', '2T1A0952.jpg', '2T1A1217.jpg',
                     '2T1A1302.jpg', '2T1A1567.jpg', '2T1A2112.jpg', 'DSC02196.jpg', 
                     'DSC02575.jpg', 'DSC02699.jpg', '2T1A0596.jpg', '2T1A0955.jpg', 
                     '2T1A1225.jpg', '2T1A1306.jpg', '2T1A1642.jpg', '2T1A2123.jpg', 
                     'DSC02216.jpg', 'DSC02583.jpg', 'DSC02733.jpg', '2T1A0601.jpg', 
                     '2T1A0957.jpg', '2T1A1227.jpg', '2T1A1320.jpg', '2T1A1649.jpg', 
                     '2T1A2128.jpg', 'DSC02262.jpg', 'DSC02589.jpg', 'DSC02736.jpg', 
                     '2T1A0604.jpg', '2T1A0981.jpg', '2T1A1235.jpg', '2T1A1325.jpg', 
                     '2T1A1659.jpg', '2T1A2132.jpg', 'DSC02312.jpg', 'DSC02595.jpg', 
                     'DSC02775.jpg', '2T1A0605.jpg', '2T1A1013.jpg', '2T1A1242.jpg', 
                     '2T1A1327.jpg', '2T1A1733.jpg', '2T1A2139.jpg', 'DSC02392.jpg', 
                     'DSC02599.jpg', 'DSC02804.jpg', '2T1A0651.jpg', '2T1A1016.jpg', 
                     '2T1A1247.jpg', '2T1A1336.jpg', '2T1A1743.jpg', '2T1A2147.jpg', 
                     'DSC02400.jpg', 'DSC02608.jpg', 'DSC02834.jpg', '2T1A0655.jpg', 
                     '2T1A1021.jpg', '2T1A1250.jpg', '2T1A1342.jpg', '2T1A1774.jpg', 
                     '2T1A2155.jpg', 'DSC02415.jpg', 'DSC02612.jpg', 'DSC02890.jpg', 
                     '2T1A0659.jpg', '2T1A1034.jpg', '2T1A1261.jpg', '2T1A1357.jpg', 
                     '2T1A1842.jpg', '2T1A2170.jpg', 'DSC02461.jpg', 'DSC02615.jpg', 
                     'DSC02901.jpg', '2T1A0721.jpg', '2T1A1067.jpg', '2T1A1263.jpg', 
                     '2T1A1374.jpg', '2T1A1872.jpg', '2T1A2174.jpg', 'DSC02472.jpg', 
                     'DSC02620.jpg', 'DSC02904.jpg', '2T1A0726.jpg', '2T1A1075.jpg', 
                     '2T1A1268.jpg', '2T1A1382.jpg', '2T1A1893.jpg', '2T1A2186.jpg', 
                     'DSC02487.jpg', 'DSC02624.jpg', 'DSC02919.jpg', '2T1A0757.jpg', 
                     '2T1A1093.jpg', '2T1A1278.jpg', '2T1A1390.jpg', '2T1A1896.jpg', 
                     '2T1A2210.jpg', 'DSC02496.jpg', 'DSC02628.jpg', 'DSC02933.jpg', 
                     '2T1A0802.jpg', '2T1A1106.jpg', '2T1A1283.jpg', '2T1A1392.jpg', 
                     '2T1A1943.jpg', '2T1A2221.jpg', 'DSC02523.jpg', 'DSC02636.jpg', 
                     'DSC02941.jpg', '2T1A0817.jpg', '2T1A1126.jpg', '2T1A1287.jpg', 
                     '2T1A1410.jpg', '2T1A2056.jpg', '2T1A2227.jpg', 'DSC02547.jpg', 
                     'DSC02641.jpg', 'DSC02955.jpg', '2T1A0863.jpg', '2T1A1133.jpg', 
                     '2T1A1288.jpg', '2T1A1442.jpg', '2T1A2061.jpg', '2T1A3675.jpg', 
                     'DSC02550.jpg', 'DSC02647.jpg', 'DSC02965.jpg', '2T1A0866.jpg', 
                     '2T1A1140.jpg', '2T1A1296.jpg', '2T1A1453.jpg', '2T1A2064.jpg', 
                     '2T1A3701.jpg', 'DSC02552.jpg', 'DSC02651.jpg', 'DSC02982.jpg', 
                     '2T1A0915.jpg', '2T1A1148.jpg', '2T1A1298.jpg', '2T1A1489.jpg', 
                     '2T1A2075.jpg', '2T1A3707.jpg', 'DSC02557.jpg', 'DSC02661.jpg', 
                     'DSC02986.jpg', '2T1A0918.jpg', '2T1A1155.jpg', '2T1A1300.jpg', 
                     '2T1A1534.jpg', '2T1A2079.jpg', '2T1A3710.jpg', 'DSC02561.jpg', 
                     'DSC02665.jpg', 'DSC03646.jpg'];

  var carouselInner = document.getElementById("carouselInner");

  for (var i = 0; i < imageUrls.length; i++) {
    var newCarouselItem = document.createElement("div");
    newCarouselItem.classList.add("carousel-item");

    var newImage = document.createElement("img");
    newImage.src = 'img/photo/'+imageUrls[i];
    newImage.classList.add("d-block", "w-100");
    
    newCarouselItem.appendChild(newImage);
    carouselInner.appendChild(newCarouselItem);
  }
  </script>

</body>

</html>
